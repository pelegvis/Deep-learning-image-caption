{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/tamirdh/Deep-learning-image-caption/blob/master/image_captioning_deep_learning_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJ_DSjOA83Oe"
   },
   "source": [
    "# Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yandex/DLW2021/davidhay/Deep-learning-image-caption\n",
      "/home/yandex/DLW2021/davidhay/Deep-learning-image-caption\n",
      "total 6596\n",
      "drwxr-xr-x 4 davidhay math1    4096 Aug 22 22:00 coco\n",
      "-rw-r--r-- 1 davidhay math1 6654519 Aug 22 00:54 image_captioning_deep_learning_workshop.ipynb\n",
      "-rw-r--r-- 1 davidhay math1      74 Aug 20 22:36 README.md\n",
      "-rw-r--r-- 1 davidhay math1      15 Aug 21 19:14 test.py\n",
      "-rw-r--r-- 1 davidhay math1     197 Aug 22 01:02 test.slurm\n",
      "-rw-r--r-- 1 davidhay math1   45488 Aug 21 18:55 vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "%cd /home/yandex/DLW2021/davidhay/Deep-learning-image-caption/\n",
    "!pwd\n",
    "!ls -l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IkLieRxMxqdZ"
   },
   "outputs": [],
   "source": [
    "!rm -rf coco\n",
    "!mkdir coco\n",
    "%cd coco\n",
    "!mkdir images\n",
    "%cd images\n",
    "!mkdir train2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -c http://images.cocodataset.org/zips/train2017.zip\n",
    "!wget -c http://images.cocodataset.org/zips/val2017.zip\n",
    "\n",
    "!unzip train2017.zip\n",
    "!unzip val2017.zip\n",
    "\n",
    "!rm train2017.zip\n",
    "!rm val2017.zip\n",
    "\n",
    "%cd ../\n",
    "!wget -c http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "\n",
    "!unzip annotations_trainval2017.zip\n",
    "\n",
    "!rm annotations_trainval2017.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEZMYzlIxCwV"
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnc6rR2P-ck-"
   },
   "source": [
    "## Imports and Vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6YUFskmKSJlD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self,freq_threshold):\n",
    "        #setting the pre-reserved tokens int to string tokens\n",
    "        # PAD- padding symbol\n",
    "        # SOS- Start of Sentence\n",
    "        # EOS- end of sentence\n",
    "        # UNK- unknown word (unknown\\ below threshold)\n",
    "        self.itos = {0:\"<PAD>\",1:\"<SOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n",
    "        #string to int tokens\n",
    "        #its reverse dict self.itos\n",
    "        self.stoi = {v:k for k,v in self.itos.items()}\n",
    "        self.freq_threshold = freq_threshold\n",
    "        \n",
    "    def __len__(self):\n",
    "      return len(self.itos)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        return [token.text.lower() for token in spacy_eng.tokenizer(text)]\n",
    "    \n",
    "    def build_vocab(self, sentence_list):\n",
    "        frequencies = Counter()\n",
    "        idx = 4\n",
    "        for index,sentence in enumerate(sentence_list):\n",
    "\n",
    "            for word in self.tokenize(sentence):\n",
    "                frequencies[word] += 1\n",
    "                \n",
    "                #add the word to the vocab if it reaches minum frequecy threshold\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    if idx > 0 and idx % 1000==0:\n",
    "                        print(f\"Added {idx} words to vocab\")\n",
    "                    idx += 1\n",
    "            if index>0 and index%1000==0:\n",
    "                print(f\"Iterated {index} sentences\")\n",
    "             \n",
    "\n",
    "        print(f\"Done, added {idx-1} words to vocabulary\")\n",
    "    \n",
    "    def numericalize(self,text):\n",
    "        \"\"\" For each word in the text corresponding index token for that word form the vocab built as list \"\"\"\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        result = [ self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text ]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkgLLalGAHXn"
   },
   "source": [
    "## Dataset custom class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Y19oWTAsWmEC"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "class COCODataset(Dataset):\n",
    "    \"\"\"\n",
    "    COCODataset\n",
    "    \"\"\"\n",
    "    def __init__(self,root_dir,annotation_file,transform=None,freq_threshold=5,\n",
    "                 load_vocab=False, vocab_loc = \"vocab.pkl\"):\n",
    "      \"\"\"\n",
    "      can use load_vocab to use a previously created vocabulary (time saving feature)\n",
    "      freq_threshold: words with a count below this number will be marked as <UNK>\n",
    "      \"\"\"\n",
    "      self.root_dir = root_dir\n",
    "      self.coco = COCO(annotation_file)\n",
    "      self.transform = transform\n",
    "      self.cap_max_size = 0\n",
    "      #Get image and caption colum from the dataframe\n",
    "      self.imgs = []\n",
    "      self.captions = []\n",
    "      for idx,ann in enumerate(self.coco.anns.values()):\n",
    "        self.imgs.append(self.coco.loadImgs((ann['image_id']))[0][\"file_name\"])\n",
    "        self.captions.append(ann['caption'])\n",
    "        if (idx) % 1000 == 0 and idx>0:\n",
    "          print(f\"Processed {idx} images and captions\")\n",
    "      print(\"Finished processing images and captions\")\n",
    "      print(f\"Got:{len(set(self.imgs))} pictures with {len(self.captions)} captions!\")\n",
    "      \n",
    "      #Initialize vocabulary and build vocab\n",
    "      if load_vocab:\n",
    "        with open(vocab_loc, \"rb\") as source:\n",
    "          self.vocab = pickle.load(source)\n",
    "        print(f\"Loaded vocabulary from {vocab_loc}\")\n",
    "      \n",
    "      else:\n",
    "        print(\"Build vocabulary\")\n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocab(self.captions)\n",
    "        print(\"Finished building vocabulary\")\n",
    "        with open(vocab_loc, \"wb\") as dest:\n",
    "          pickle.dump(self.vocab, dest)\n",
    "      \n",
    "      print(f\"Using {len(self.vocab)} words\")\n",
    "    \n",
    "    def __len__(self):\n",
    "      return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "      caption = self.captions[idx]\n",
    "      img_name = self.imgs[idx]\n",
    "      img_location = os.path.join(self.root_dir,img_name)\n",
    "      img = Image.open(img_location).convert(\"RGB\")\n",
    "      \n",
    "      #apply the transfromation to the image\n",
    "      if self.transform:\n",
    "          img = self.transform(img)\n",
    "      \n",
    "      #numericalize the caption text\n",
    "      caption_vec = [self.vocab.stoi[\"<SOS>\"]]\n",
    "      caption_vec.extend(self.vocab.numericalize(caption))\n",
    "      caption_vec.append(self.vocab.stoi[\"<EOS>\"])\n",
    "      \n",
    "      return img, torch.tensor(caption_vec,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDqCBk-KCBkw"
   },
   "source": [
    "## Dataloader creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pNHTCZ8ZcXGQ"
   },
   "outputs": [],
   "source": [
    "# define a transformation to add some noise and variance to our images\n",
    "transformation = transforms.Compose([transforms.Resize((512,512), Image.NEAREST),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.RandomVerticalFlip(),\n",
    "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "w_flS7fBeC0r"
   },
   "outputs": [],
   "source": [
    "class CapsCollate:\n",
    "    \"\"\"\n",
    "    Collate to apply the padding to the captions with dataloader\n",
    "    \"\"\"\n",
    "    def __init__(self,pad_idx,batch_first=False, vec_len=-1):\n",
    "        self.pad_idx = pad_idx\n",
    "        self.batch_first = batch_first\n",
    "        self.vec_len = vec_len + 2 # adding the <SOS> and <EOS>\n",
    "        assert self.vec_len > 0, \"Vector length must be positive integer\"\n",
    "    \n",
    "    def __call__(self,batch):\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "        imgs = torch.cat(imgs,dim=0)\n",
    "        targets_list = list()\n",
    "        for item in batch:\n",
    "            # item = (img:Image, caption:tensor)\n",
    "            addition = self.vec_len-len(item[1])\n",
    "            padded_target = torch.cat((item[1], torch.empty(addition,dtype=torch.long).fill_(pad_idx)),dim=0)\n",
    "            targets_list.append(padded_target)\n",
    "            #print(f\"GOT:{item[1]}, {item[1].type()}\\nAdding:{addition}\\nPADDED:{padded_target}\\n{padded_target.type()}\")\n",
    "        targets = torch.stack(targets_list,0)\n",
    "        #print(f\"Targets shape:{targets.shape}\")\n",
    "        return imgs,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "igD_-Qs7oFTz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.55s)\n",
      "creating index...\n",
      "index created!\n",
      "Processed 1000 images and captions\n",
      "Processed 2000 images and captions\n",
      "Processed 3000 images and captions\n",
      "Processed 4000 images and captions\n",
      "Processed 5000 images and captions\n",
      "Processed 6000 images and captions\n",
      "Processed 7000 images and captions\n",
      "Processed 8000 images and captions\n",
      "Processed 9000 images and captions\n",
      "Processed 10000 images and captions\n",
      "Processed 11000 images and captions\n",
      "Processed 12000 images and captions\n",
      "Processed 13000 images and captions\n",
      "Processed 14000 images and captions\n",
      "Processed 15000 images and captions\n",
      "Processed 16000 images and captions\n",
      "Processed 17000 images and captions\n",
      "Processed 18000 images and captions\n",
      "Processed 19000 images and captions\n",
      "Processed 20000 images and captions\n",
      "Processed 21000 images and captions\n",
      "Processed 22000 images and captions\n",
      "Processed 23000 images and captions\n",
      "Processed 24000 images and captions\n",
      "Processed 25000 images and captions\n",
      "Processed 26000 images and captions\n",
      "Processed 27000 images and captions\n",
      "Processed 28000 images and captions\n",
      "Processed 29000 images and captions\n",
      "Processed 30000 images and captions\n",
      "Processed 31000 images and captions\n",
      "Processed 32000 images and captions\n",
      "Processed 33000 images and captions\n",
      "Processed 34000 images and captions\n",
      "Processed 35000 images and captions\n",
      "Processed 36000 images and captions\n",
      "Processed 37000 images and captions\n",
      "Processed 38000 images and captions\n",
      "Processed 39000 images and captions\n",
      "Processed 40000 images and captions\n",
      "Processed 41000 images and captions\n",
      "Processed 42000 images and captions\n",
      "Processed 43000 images and captions\n",
      "Processed 44000 images and captions\n",
      "Processed 45000 images and captions\n",
      "Processed 46000 images and captions\n",
      "Processed 47000 images and captions\n",
      "Processed 48000 images and captions\n",
      "Processed 49000 images and captions\n",
      "Processed 50000 images and captions\n",
      "Processed 51000 images and captions\n",
      "Processed 52000 images and captions\n",
      "Processed 53000 images and captions\n",
      "Processed 54000 images and captions\n",
      "Processed 55000 images and captions\n",
      "Processed 56000 images and captions\n",
      "Processed 57000 images and captions\n",
      "Processed 58000 images and captions\n",
      "Processed 59000 images and captions\n",
      "Processed 60000 images and captions\n",
      "Processed 61000 images and captions\n",
      "Processed 62000 images and captions\n",
      "Processed 63000 images and captions\n",
      "Processed 64000 images and captions\n",
      "Processed 65000 images and captions\n",
      "Processed 66000 images and captions\n",
      "Processed 67000 images and captions\n",
      "Processed 68000 images and captions\n",
      "Processed 69000 images and captions\n",
      "Processed 70000 images and captions\n",
      "Processed 71000 images and captions\n",
      "Processed 72000 images and captions\n",
      "Processed 73000 images and captions\n",
      "Processed 74000 images and captions\n",
      "Processed 75000 images and captions\n",
      "Processed 76000 images and captions\n",
      "Processed 77000 images and captions\n",
      "Processed 78000 images and captions\n",
      "Processed 79000 images and captions\n",
      "Processed 80000 images and captions\n",
      "Processed 81000 images and captions\n",
      "Processed 82000 images and captions\n",
      "Processed 83000 images and captions\n",
      "Processed 84000 images and captions\n",
      "Processed 85000 images and captions\n",
      "Processed 86000 images and captions\n",
      "Processed 87000 images and captions\n",
      "Processed 88000 images and captions\n",
      "Processed 89000 images and captions\n",
      "Processed 90000 images and captions\n",
      "Processed 91000 images and captions\n",
      "Processed 92000 images and captions\n",
      "Processed 93000 images and captions\n",
      "Processed 94000 images and captions\n",
      "Processed 95000 images and captions\n",
      "Processed 96000 images and captions\n",
      "Processed 97000 images and captions\n",
      "Processed 98000 images and captions\n",
      "Processed 99000 images and captions\n",
      "Processed 100000 images and captions\n",
      "Processed 101000 images and captions\n",
      "Processed 102000 images and captions\n",
      "Processed 103000 images and captions\n",
      "Processed 104000 images and captions\n",
      "Processed 105000 images and captions\n",
      "Processed 106000 images and captions\n",
      "Processed 107000 images and captions\n",
      "Processed 108000 images and captions\n",
      "Processed 109000 images and captions\n",
      "Processed 110000 images and captions\n",
      "Processed 111000 images and captions\n",
      "Processed 112000 images and captions\n",
      "Processed 113000 images and captions\n",
      "Processed 114000 images and captions\n",
      "Processed 115000 images and captions\n",
      "Processed 116000 images and captions\n",
      "Processed 117000 images and captions\n",
      "Processed 118000 images and captions\n",
      "Processed 119000 images and captions\n",
      "Processed 120000 images and captions\n",
      "Processed 121000 images and captions\n",
      "Processed 122000 images and captions\n",
      "Processed 123000 images and captions\n",
      "Processed 124000 images and captions\n",
      "Processed 125000 images and captions\n",
      "Processed 126000 images and captions\n",
      "Processed 127000 images and captions\n",
      "Processed 128000 images and captions\n",
      "Processed 129000 images and captions\n",
      "Processed 130000 images and captions\n",
      "Processed 131000 images and captions\n",
      "Processed 132000 images and captions\n",
      "Processed 133000 images and captions\n",
      "Processed 134000 images and captions\n",
      "Processed 135000 images and captions\n",
      "Processed 136000 images and captions\n",
      "Processed 137000 images and captions\n",
      "Processed 138000 images and captions\n",
      "Processed 139000 images and captions\n",
      "Processed 140000 images and captions\n",
      "Processed 141000 images and captions\n",
      "Processed 142000 images and captions\n",
      "Processed 143000 images and captions\n",
      "Processed 144000 images and captions\n",
      "Processed 145000 images and captions\n",
      "Processed 146000 images and captions\n",
      "Processed 147000 images and captions\n",
      "Processed 148000 images and captions\n",
      "Processed 149000 images and captions\n",
      "Processed 150000 images and captions\n",
      "Processed 151000 images and captions\n",
      "Processed 152000 images and captions\n",
      "Processed 153000 images and captions\n",
      "Processed 154000 images and captions\n",
      "Processed 155000 images and captions\n",
      "Processed 156000 images and captions\n",
      "Processed 157000 images and captions\n",
      "Processed 158000 images and captions\n",
      "Processed 159000 images and captions\n",
      "Processed 160000 images and captions\n",
      "Processed 161000 images and captions\n",
      "Processed 162000 images and captions\n",
      "Processed 163000 images and captions\n",
      "Processed 164000 images and captions\n",
      "Processed 165000 images and captions\n",
      "Processed 166000 images and captions\n",
      "Processed 167000 images and captions\n",
      "Processed 168000 images and captions\n",
      "Processed 169000 images and captions\n",
      "Processed 170000 images and captions\n",
      "Processed 171000 images and captions\n",
      "Processed 172000 images and captions\n",
      "Processed 173000 images and captions\n",
      "Processed 174000 images and captions\n",
      "Processed 175000 images and captions\n",
      "Processed 176000 images and captions\n",
      "Processed 177000 images and captions\n",
      "Processed 178000 images and captions\n",
      "Processed 179000 images and captions\n",
      "Processed 180000 images and captions\n",
      "Processed 181000 images and captions\n",
      "Processed 182000 images and captions\n",
      "Processed 183000 images and captions\n",
      "Processed 184000 images and captions\n",
      "Processed 185000 images and captions\n",
      "Processed 186000 images and captions\n",
      "Processed 187000 images and captions\n",
      "Processed 188000 images and captions\n",
      "Processed 189000 images and captions\n",
      "Processed 190000 images and captions\n",
      "Processed 191000 images and captions\n",
      "Processed 192000 images and captions\n",
      "Processed 193000 images and captions\n",
      "Processed 194000 images and captions\n",
      "Processed 195000 images and captions\n",
      "Processed 196000 images and captions\n",
      "Processed 197000 images and captions\n",
      "Processed 198000 images and captions\n",
      "Processed 199000 images and captions\n",
      "Processed 200000 images and captions\n",
      "Processed 201000 images and captions\n",
      "Processed 202000 images and captions\n",
      "Processed 203000 images and captions\n",
      "Processed 204000 images and captions\n",
      "Processed 205000 images and captions\n",
      "Processed 206000 images and captions\n",
      "Processed 207000 images and captions\n",
      "Processed 208000 images and captions\n",
      "Processed 209000 images and captions\n",
      "Processed 210000 images and captions\n",
      "Processed 211000 images and captions\n",
      "Processed 212000 images and captions\n",
      "Processed 213000 images and captions\n",
      "Processed 214000 images and captions\n",
      "Processed 215000 images and captions\n",
      "Processed 216000 images and captions\n",
      "Processed 217000 images and captions\n",
      "Processed 218000 images and captions\n",
      "Processed 219000 images and captions\n",
      "Processed 220000 images and captions\n",
      "Processed 221000 images and captions\n",
      "Processed 222000 images and captions\n",
      "Processed 223000 images and captions\n",
      "Processed 224000 images and captions\n",
      "Processed 225000 images and captions\n",
      "Processed 226000 images and captions\n",
      "Processed 227000 images and captions\n",
      "Processed 228000 images and captions\n",
      "Processed 229000 images and captions\n",
      "Processed 230000 images and captions\n",
      "Processed 231000 images and captions\n",
      "Processed 232000 images and captions\n",
      "Processed 233000 images and captions\n",
      "Processed 234000 images and captions\n",
      "Processed 235000 images and captions\n",
      "Processed 236000 images and captions\n",
      "Processed 237000 images and captions\n",
      "Processed 238000 images and captions\n",
      "Processed 239000 images and captions\n",
      "Processed 240000 images and captions\n",
      "Processed 241000 images and captions\n",
      "Processed 242000 images and captions\n",
      "Processed 243000 images and captions\n",
      "Processed 244000 images and captions\n",
      "Processed 245000 images and captions\n",
      "Processed 246000 images and captions\n",
      "Processed 247000 images and captions\n",
      "Processed 248000 images and captions\n",
      "Processed 249000 images and captions\n",
      "Processed 250000 images and captions\n",
      "Processed 251000 images and captions\n",
      "Processed 252000 images and captions\n",
      "Processed 253000 images and captions\n",
      "Processed 254000 images and captions\n",
      "Processed 255000 images and captions\n",
      "Processed 256000 images and captions\n",
      "Processed 257000 images and captions\n",
      "Processed 258000 images and captions\n",
      "Processed 259000 images and captions\n",
      "Processed 260000 images and captions\n",
      "Processed 261000 images and captions\n",
      "Processed 262000 images and captions\n",
      "Processed 263000 images and captions\n",
      "Processed 264000 images and captions\n",
      "Processed 265000 images and captions\n",
      "Processed 266000 images and captions\n",
      "Processed 267000 images and captions\n",
      "Processed 268000 images and captions\n",
      "Processed 269000 images and captions\n",
      "Processed 270000 images and captions\n",
      "Processed 271000 images and captions\n",
      "Processed 272000 images and captions\n",
      "Processed 273000 images and captions\n",
      "Processed 274000 images and captions\n",
      "Processed 275000 images and captions\n",
      "Processed 276000 images and captions\n",
      "Processed 277000 images and captions\n",
      "Processed 278000 images and captions\n",
      "Processed 279000 images and captions\n",
      "Processed 280000 images and captions\n",
      "Processed 281000 images and captions\n",
      "Processed 282000 images and captions\n",
      "Processed 283000 images and captions\n",
      "Processed 284000 images and captions\n",
      "Processed 285000 images and captions\n",
      "Processed 286000 images and captions\n",
      "Processed 287000 images and captions\n",
      "Processed 288000 images and captions\n",
      "Processed 289000 images and captions\n",
      "Processed 290000 images and captions\n",
      "Processed 291000 images and captions\n",
      "Processed 292000 images and captions\n",
      "Processed 293000 images and captions\n",
      "Processed 294000 images and captions\n",
      "Processed 295000 images and captions\n",
      "Processed 296000 images and captions\n",
      "Processed 297000 images and captions\n",
      "Processed 298000 images and captions\n",
      "Processed 299000 images and captions\n",
      "Processed 300000 images and captions\n",
      "Processed 301000 images and captions\n",
      "Processed 302000 images and captions\n",
      "Processed 303000 images and captions\n",
      "Processed 304000 images and captions\n",
      "Processed 305000 images and captions\n",
      "Processed 306000 images and captions\n",
      "Processed 307000 images and captions\n",
      "Processed 308000 images and captions\n",
      "Processed 309000 images and captions\n",
      "Processed 310000 images and captions\n",
      "Processed 311000 images and captions\n",
      "Processed 312000 images and captions\n",
      "Processed 313000 images and captions\n",
      "Processed 314000 images and captions\n",
      "Processed 315000 images and captions\n",
      "Processed 316000 images and captions\n",
      "Processed 317000 images and captions\n",
      "Processed 318000 images and captions\n",
      "Processed 319000 images and captions\n",
      "Processed 320000 images and captions\n",
      "Processed 321000 images and captions\n",
      "Processed 322000 images and captions\n",
      "Processed 323000 images and captions\n",
      "Processed 324000 images and captions\n",
      "Processed 325000 images and captions\n",
      "Processed 326000 images and captions\n",
      "Processed 327000 images and captions\n",
      "Processed 328000 images and captions\n",
      "Processed 329000 images and captions\n",
      "Processed 330000 images and captions\n",
      "Processed 331000 images and captions\n",
      "Processed 332000 images and captions\n",
      "Processed 333000 images and captions\n",
      "Processed 334000 images and captions\n",
      "Processed 335000 images and captions\n",
      "Processed 336000 images and captions\n",
      "Processed 337000 images and captions\n",
      "Processed 338000 images and captions\n",
      "Processed 339000 images and captions\n",
      "Processed 340000 images and captions\n",
      "Processed 341000 images and captions\n",
      "Processed 342000 images and captions\n",
      "Processed 343000 images and captions\n",
      "Processed 344000 images and captions\n",
      "Processed 345000 images and captions\n",
      "Processed 346000 images and captions\n",
      "Processed 347000 images and captions\n",
      "Processed 348000 images and captions\n",
      "Processed 349000 images and captions\n",
      "Processed 350000 images and captions\n",
      "Processed 351000 images and captions\n",
      "Processed 352000 images and captions\n",
      "Processed 353000 images and captions\n",
      "Processed 354000 images and captions\n",
      "Processed 355000 images and captions\n",
      "Processed 356000 images and captions\n",
      "Processed 357000 images and captions\n",
      "Processed 358000 images and captions\n",
      "Processed 359000 images and captions\n",
      "Processed 360000 images and captions\n",
      "Processed 361000 images and captions\n",
      "Processed 362000 images and captions\n",
      "Processed 363000 images and captions\n",
      "Processed 364000 images and captions\n",
      "Processed 365000 images and captions\n",
      "Processed 366000 images and captions\n",
      "Processed 367000 images and captions\n",
      "Processed 368000 images and captions\n",
      "Processed 369000 images and captions\n",
      "Processed 370000 images and captions\n",
      "Processed 371000 images and captions\n",
      "Processed 372000 images and captions\n",
      "Processed 373000 images and captions\n",
      "Processed 374000 images and captions\n",
      "Processed 375000 images and captions\n",
      "Processed 376000 images and captions\n",
      "Processed 377000 images and captions\n",
      "Processed 378000 images and captions\n",
      "Processed 379000 images and captions\n",
      "Processed 380000 images and captions\n",
      "Processed 381000 images and captions\n",
      "Processed 382000 images and captions\n",
      "Processed 383000 images and captions\n",
      "Processed 384000 images and captions\n",
      "Processed 385000 images and captions\n",
      "Processed 386000 images and captions\n",
      "Processed 387000 images and captions\n",
      "Processed 388000 images and captions\n",
      "Processed 389000 images and captions\n",
      "Processed 390000 images and captions\n",
      "Processed 391000 images and captions\n",
      "Processed 392000 images and captions\n",
      "Processed 393000 images and captions\n",
      "Processed 394000 images and captions\n",
      "Processed 395000 images and captions\n",
      "Processed 396000 images and captions\n",
      "Processed 397000 images and captions\n",
      "Processed 398000 images and captions\n",
      "Processed 399000 images and captions\n",
      "Processed 400000 images and captions\n",
      "Processed 401000 images and captions\n",
      "Processed 402000 images and captions\n",
      "Processed 403000 images and captions\n",
      "Processed 404000 images and captions\n",
      "Processed 405000 images and captions\n",
      "Processed 406000 images and captions\n",
      "Processed 407000 images and captions\n",
      "Processed 408000 images and captions\n",
      "Processed 409000 images and captions\n",
      "Processed 410000 images and captions\n",
      "Processed 411000 images and captions\n",
      "Processed 412000 images and captions\n",
      "Processed 413000 images and captions\n",
      "Processed 414000 images and captions\n",
      "Processed 415000 images and captions\n",
      "Processed 416000 images and captions\n",
      "Processed 417000 images and captions\n",
      "Processed 418000 images and captions\n",
      "Processed 419000 images and captions\n",
      "Processed 420000 images and captions\n",
      "Processed 421000 images and captions\n",
      "Processed 422000 images and captions\n",
      "Processed 423000 images and captions\n",
      "Processed 424000 images and captions\n",
      "Processed 425000 images and captions\n",
      "Processed 426000 images and captions\n",
      "Processed 427000 images and captions\n",
      "Processed 428000 images and captions\n",
      "Processed 429000 images and captions\n",
      "Processed 430000 images and captions\n",
      "Processed 431000 images and captions\n",
      "Processed 432000 images and captions\n",
      "Processed 433000 images and captions\n",
      "Processed 434000 images and captions\n",
      "Processed 435000 images and captions\n",
      "Processed 436000 images and captions\n",
      "Processed 437000 images and captions\n",
      "Processed 438000 images and captions\n",
      "Processed 439000 images and captions\n",
      "Processed 440000 images and captions\n",
      "Processed 441000 images and captions\n",
      "Processed 442000 images and captions\n",
      "Processed 443000 images and captions\n",
      "Processed 444000 images and captions\n",
      "Processed 445000 images and captions\n",
      "Processed 446000 images and captions\n",
      "Processed 447000 images and captions\n",
      "Processed 448000 images and captions\n",
      "Processed 449000 images and captions\n",
      "Processed 450000 images and captions\n",
      "Processed 451000 images and captions\n",
      "Processed 452000 images and captions\n",
      "Processed 453000 images and captions\n",
      "Processed 454000 images and captions\n",
      "Processed 455000 images and captions\n",
      "Processed 456000 images and captions\n",
      "Processed 457000 images and captions\n",
      "Processed 458000 images and captions\n",
      "Processed 459000 images and captions\n",
      "Processed 460000 images and captions\n",
      "Processed 461000 images and captions\n",
      "Processed 462000 images and captions\n",
      "Processed 463000 images and captions\n",
      "Processed 464000 images and captions\n",
      "Processed 465000 images and captions\n",
      "Processed 466000 images and captions\n",
      "Processed 467000 images and captions\n",
      "Processed 468000 images and captions\n",
      "Processed 469000 images and captions\n",
      "Processed 470000 images and captions\n",
      "Processed 471000 images and captions\n",
      "Processed 472000 images and captions\n",
      "Processed 473000 images and captions\n",
      "Processed 474000 images and captions\n",
      "Processed 475000 images and captions\n",
      "Processed 476000 images and captions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 477000 images and captions\n",
      "Processed 478000 images and captions\n",
      "Processed 479000 images and captions\n",
      "Processed 480000 images and captions\n",
      "Processed 481000 images and captions\n",
      "Processed 482000 images and captions\n",
      "Processed 483000 images and captions\n",
      "Processed 484000 images and captions\n",
      "Processed 485000 images and captions\n",
      "Processed 486000 images and captions\n",
      "Processed 487000 images and captions\n",
      "Processed 488000 images and captions\n",
      "Processed 489000 images and captions\n",
      "Processed 490000 images and captions\n",
      "Processed 491000 images and captions\n",
      "Processed 492000 images and captions\n",
      "Processed 493000 images and captions\n",
      "Processed 494000 images and captions\n",
      "Processed 495000 images and captions\n",
      "Processed 496000 images and captions\n",
      "Processed 497000 images and captions\n",
      "Processed 498000 images and captions\n",
      "Processed 499000 images and captions\n",
      "Processed 500000 images and captions\n",
      "Processed 501000 images and captions\n",
      "Processed 502000 images and captions\n",
      "Processed 503000 images and captions\n",
      "Processed 504000 images and captions\n",
      "Processed 505000 images and captions\n",
      "Processed 506000 images and captions\n",
      "Processed 507000 images and captions\n",
      "Processed 508000 images and captions\n",
      "Processed 509000 images and captions\n",
      "Processed 510000 images and captions\n",
      "Processed 511000 images and captions\n",
      "Processed 512000 images and captions\n",
      "Processed 513000 images and captions\n",
      "Processed 514000 images and captions\n",
      "Processed 515000 images and captions\n",
      "Processed 516000 images and captions\n",
      "Processed 517000 images and captions\n",
      "Processed 518000 images and captions\n",
      "Processed 519000 images and captions\n",
      "Processed 520000 images and captions\n",
      "Processed 521000 images and captions\n",
      "Processed 522000 images and captions\n",
      "Processed 523000 images and captions\n",
      "Processed 524000 images and captions\n",
      "Processed 525000 images and captions\n",
      "Processed 526000 images and captions\n",
      "Processed 527000 images and captions\n",
      "Processed 528000 images and captions\n",
      "Processed 529000 images and captions\n",
      "Processed 530000 images and captions\n",
      "Processed 531000 images and captions\n",
      "Processed 532000 images and captions\n",
      "Processed 533000 images and captions\n",
      "Processed 534000 images and captions\n",
      "Processed 535000 images and captions\n",
      "Processed 536000 images and captions\n",
      "Processed 537000 images and captions\n",
      "Processed 538000 images and captions\n",
      "Processed 539000 images and captions\n",
      "Processed 540000 images and captions\n",
      "Processed 541000 images and captions\n",
      "Processed 542000 images and captions\n",
      "Processed 543000 images and captions\n",
      "Processed 544000 images and captions\n",
      "Processed 545000 images and captions\n",
      "Processed 546000 images and captions\n",
      "Processed 547000 images and captions\n",
      "Processed 548000 images and captions\n",
      "Processed 549000 images and captions\n",
      "Processed 550000 images and captions\n",
      "Processed 551000 images and captions\n",
      "Processed 552000 images and captions\n",
      "Processed 553000 images and captions\n",
      "Processed 554000 images and captions\n",
      "Processed 555000 images and captions\n",
      "Processed 556000 images and captions\n",
      "Processed 557000 images and captions\n",
      "Processed 558000 images and captions\n",
      "Processed 559000 images and captions\n",
      "Processed 560000 images and captions\n",
      "Processed 561000 images and captions\n",
      "Processed 562000 images and captions\n",
      "Processed 563000 images and captions\n",
      "Processed 564000 images and captions\n",
      "Processed 565000 images and captions\n",
      "Processed 566000 images and captions\n",
      "Processed 567000 images and captions\n",
      "Processed 568000 images and captions\n",
      "Processed 569000 images and captions\n",
      "Processed 570000 images and captions\n",
      "Processed 571000 images and captions\n",
      "Processed 572000 images and captions\n",
      "Processed 573000 images and captions\n",
      "Processed 574000 images and captions\n",
      "Processed 575000 images and captions\n",
      "Processed 576000 images and captions\n",
      "Processed 577000 images and captions\n",
      "Processed 578000 images and captions\n",
      "Processed 579000 images and captions\n",
      "Processed 580000 images and captions\n",
      "Processed 581000 images and captions\n",
      "Processed 582000 images and captions\n",
      "Processed 583000 images and captions\n",
      "Processed 584000 images and captions\n",
      "Processed 585000 images and captions\n",
      "Processed 586000 images and captions\n",
      "Processed 587000 images and captions\n",
      "Processed 588000 images and captions\n",
      "Processed 589000 images and captions\n",
      "Processed 590000 images and captions\n",
      "Processed 591000 images and captions\n",
      "Finished processing images and captions\n",
      "Got:118287 pictures with 591753 captions!\n",
      "Build vocabulary\n",
      "Iterated 1000 sentences\n",
      "Iterated 2000 sentences\n",
      "Iterated 3000 sentences\n",
      "Iterated 4000 sentences\n",
      "Iterated 5000 sentences\n",
      "Added 1000 words to vocab\n",
      "Iterated 6000 sentences\n",
      "Iterated 7000 sentences\n",
      "Iterated 8000 sentences\n",
      "Iterated 9000 sentences\n",
      "Iterated 10000 sentences\n",
      "Iterated 11000 sentences\n",
      "Iterated 12000 sentences\n",
      "Iterated 13000 sentences\n",
      "Iterated 14000 sentences\n",
      "Iterated 15000 sentences\n",
      "Iterated 16000 sentences\n",
      "Iterated 17000 sentences\n",
      "Iterated 18000 sentences\n",
      "Iterated 19000 sentences\n",
      "Iterated 20000 sentences\n",
      "Iterated 21000 sentences\n",
      "Added 2000 words to vocab\n",
      "Iterated 22000 sentences\n",
      "Iterated 23000 sentences\n",
      "Iterated 24000 sentences\n",
      "Iterated 25000 sentences\n",
      "Iterated 26000 sentences\n",
      "Iterated 27000 sentences\n",
      "Iterated 28000 sentences\n",
      "Iterated 29000 sentences\n",
      "Iterated 30000 sentences\n",
      "Iterated 31000 sentences\n",
      "Iterated 32000 sentences\n",
      "Iterated 33000 sentences\n",
      "Iterated 34000 sentences\n",
      "Iterated 35000 sentences\n",
      "Iterated 36000 sentences\n",
      "Iterated 37000 sentences\n",
      "Iterated 38000 sentences\n",
      "Iterated 39000 sentences\n",
      "Iterated 40000 sentences\n",
      "Iterated 41000 sentences\n",
      "Iterated 42000 sentences\n",
      "Iterated 43000 sentences\n",
      "Added 3000 words to vocab\n",
      "Iterated 44000 sentences\n",
      "Iterated 45000 sentences\n",
      "Iterated 46000 sentences\n",
      "Iterated 47000 sentences\n",
      "Iterated 48000 sentences\n",
      "Iterated 49000 sentences\n",
      "Iterated 50000 sentences\n",
      "Iterated 51000 sentences\n",
      "Iterated 52000 sentences\n",
      "Iterated 53000 sentences\n",
      "Iterated 54000 sentences\n",
      "Iterated 55000 sentences\n",
      "Iterated 56000 sentences\n",
      "Iterated 57000 sentences\n",
      "Iterated 58000 sentences\n",
      "Iterated 59000 sentences\n",
      "Iterated 60000 sentences\n",
      "Iterated 61000 sentences\n",
      "Iterated 62000 sentences\n",
      "Iterated 63000 sentences\n",
      "Iterated 64000 sentences\n",
      "Iterated 65000 sentences\n",
      "Iterated 66000 sentences\n",
      "Iterated 67000 sentences\n",
      "Iterated 68000 sentences\n",
      "Iterated 69000 sentences\n",
      "Iterated 70000 sentences\n",
      "Iterated 71000 sentences\n",
      "Iterated 72000 sentences\n",
      "Iterated 73000 sentences\n",
      "Iterated 74000 sentences\n",
      "Iterated 75000 sentences\n",
      "Iterated 76000 sentences\n",
      "Iterated 77000 sentences\n",
      "Added 4000 words to vocab\n",
      "Iterated 78000 sentences\n",
      "Iterated 79000 sentences\n",
      "Iterated 80000 sentences\n",
      "Iterated 81000 sentences\n",
      "Iterated 82000 sentences\n",
      "Iterated 83000 sentences\n",
      "Iterated 84000 sentences\n",
      "Iterated 85000 sentences\n",
      "Iterated 86000 sentences\n",
      "Iterated 87000 sentences\n",
      "Iterated 88000 sentences\n",
      "Iterated 89000 sentences\n",
      "Iterated 90000 sentences\n",
      "Iterated 91000 sentences\n",
      "Iterated 92000 sentences\n",
      "Iterated 93000 sentences\n",
      "Iterated 94000 sentences\n",
      "Iterated 95000 sentences\n",
      "Iterated 96000 sentences\n",
      "Iterated 97000 sentences\n",
      "Iterated 98000 sentences\n",
      "Iterated 99000 sentences\n",
      "Iterated 100000 sentences\n",
      "Iterated 101000 sentences\n",
      "Iterated 102000 sentences\n",
      "Iterated 103000 sentences\n",
      "Iterated 104000 sentences\n",
      "Iterated 105000 sentences\n",
      "Iterated 106000 sentences\n",
      "Iterated 107000 sentences\n",
      "Iterated 108000 sentences\n",
      "Iterated 109000 sentences\n",
      "Iterated 110000 sentences\n",
      "Iterated 111000 sentences\n",
      "Iterated 112000 sentences\n",
      "Iterated 113000 sentences\n",
      "Iterated 114000 sentences\n",
      "Iterated 115000 sentences\n",
      "Iterated 116000 sentences\n",
      "Iterated 117000 sentences\n",
      "Added 5000 words to vocab\n",
      "Iterated 118000 sentences\n",
      "Iterated 119000 sentences\n",
      "Iterated 120000 sentences\n",
      "Iterated 121000 sentences\n",
      "Iterated 122000 sentences\n",
      "Iterated 123000 sentences\n",
      "Iterated 124000 sentences\n",
      "Iterated 125000 sentences\n",
      "Iterated 126000 sentences\n",
      "Iterated 127000 sentences\n",
      "Iterated 128000 sentences\n",
      "Iterated 129000 sentences\n",
      "Iterated 130000 sentences\n",
      "Iterated 131000 sentences\n",
      "Iterated 132000 sentences\n",
      "Iterated 133000 sentences\n",
      "Iterated 134000 sentences\n",
      "Iterated 135000 sentences\n",
      "Iterated 136000 sentences\n",
      "Iterated 137000 sentences\n",
      "Iterated 138000 sentences\n",
      "Iterated 139000 sentences\n",
      "Iterated 140000 sentences\n",
      "Iterated 141000 sentences\n",
      "Iterated 142000 sentences\n",
      "Iterated 143000 sentences\n",
      "Iterated 144000 sentences\n",
      "Iterated 145000 sentences\n",
      "Iterated 146000 sentences\n",
      "Iterated 147000 sentences\n",
      "Iterated 148000 sentences\n",
      "Iterated 149000 sentences\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterated 150000 sentences\n",
      "Iterated 151000 sentences\n",
      "Iterated 152000 sentences\n",
      "Iterated 153000 sentences\n",
      "Iterated 154000 sentences\n",
      "Iterated 155000 sentences\n",
      "Iterated 156000 sentences\n",
      "Iterated 157000 sentences\n",
      "Iterated 158000 sentences\n",
      "Iterated 159000 sentences\n",
      "Iterated 160000 sentences\n",
      "Iterated 161000 sentences\n",
      "Iterated 162000 sentences\n",
      "Iterated 163000 sentences\n",
      "Iterated 164000 sentences\n",
      "Iterated 165000 sentences\n",
      "Iterated 166000 sentences\n",
      "Iterated 167000 sentences\n",
      "Iterated 168000 sentences\n",
      "Iterated 169000 sentences\n",
      "Iterated 170000 sentences\n",
      "Iterated 171000 sentences\n",
      "Added 6000 words to vocab\n",
      "Iterated 172000 sentences\n",
      "Iterated 173000 sentences\n",
      "Iterated 174000 sentences\n",
      "Iterated 175000 sentences\n",
      "Iterated 176000 sentences\n",
      "Iterated 177000 sentences\n",
      "Iterated 178000 sentences\n",
      "Iterated 179000 sentences\n",
      "Iterated 180000 sentences\n",
      "Iterated 181000 sentences\n",
      "Iterated 182000 sentences\n",
      "Iterated 183000 sentences\n",
      "Iterated 184000 sentences\n",
      "Iterated 185000 sentences\n",
      "Iterated 186000 sentences\n",
      "Iterated 187000 sentences\n",
      "Iterated 188000 sentences\n",
      "Iterated 189000 sentences\n",
      "Iterated 190000 sentences\n",
      "Iterated 191000 sentences\n",
      "Iterated 192000 sentences\n",
      "Iterated 193000 sentences\n",
      "Iterated 194000 sentences\n",
      "Iterated 195000 sentences\n",
      "Iterated 196000 sentences\n",
      "Iterated 197000 sentences\n",
      "Iterated 198000 sentences\n",
      "Iterated 199000 sentences\n",
      "Iterated 200000 sentences\n",
      "Iterated 201000 sentences\n",
      "Iterated 202000 sentences\n",
      "Iterated 203000 sentences\n",
      "Iterated 204000 sentences\n",
      "Iterated 205000 sentences\n",
      "Iterated 206000 sentences\n",
      "Iterated 207000 sentences\n",
      "Iterated 208000 sentences\n",
      "Iterated 209000 sentences\n",
      "Iterated 210000 sentences\n",
      "Iterated 211000 sentences\n",
      "Iterated 212000 sentences\n",
      "Iterated 213000 sentences\n",
      "Iterated 214000 sentences\n",
      "Iterated 215000 sentences\n",
      "Iterated 216000 sentences\n",
      "Iterated 217000 sentences\n",
      "Iterated 218000 sentences\n",
      "Iterated 219000 sentences\n",
      "Iterated 220000 sentences\n",
      "Iterated 221000 sentences\n",
      "Iterated 222000 sentences\n",
      "Iterated 223000 sentences\n",
      "Iterated 224000 sentences\n",
      "Iterated 225000 sentences\n",
      "Iterated 226000 sentences\n",
      "Iterated 227000 sentences\n",
      "Iterated 228000 sentences\n",
      "Iterated 229000 sentences\n",
      "Iterated 230000 sentences\n",
      "Iterated 231000 sentences\n",
      "Iterated 232000 sentences\n",
      "Iterated 233000 sentences\n",
      "Iterated 234000 sentences\n",
      "Iterated 235000 sentences\n",
      "Iterated 236000 sentences\n",
      "Iterated 237000 sentences\n",
      "Iterated 238000 sentences\n",
      "Iterated 239000 sentences\n",
      "Iterated 240000 sentences\n",
      "Added 7000 words to vocab\n",
      "Iterated 241000 sentences\n",
      "Iterated 242000 sentences\n",
      "Iterated 243000 sentences\n",
      "Iterated 244000 sentences\n",
      "Iterated 245000 sentences\n",
      "Iterated 246000 sentences\n",
      "Iterated 247000 sentences\n",
      "Iterated 248000 sentences\n",
      "Iterated 249000 sentences\n",
      "Iterated 250000 sentences\n",
      "Iterated 251000 sentences\n",
      "Iterated 252000 sentences\n",
      "Iterated 253000 sentences\n",
      "Iterated 254000 sentences\n",
      "Iterated 255000 sentences\n",
      "Iterated 256000 sentences\n",
      "Iterated 257000 sentences\n",
      "Iterated 258000 sentences\n",
      "Iterated 259000 sentences\n",
      "Iterated 260000 sentences\n",
      "Iterated 261000 sentences\n",
      "Iterated 262000 sentences\n",
      "Iterated 263000 sentences\n",
      "Iterated 264000 sentences\n",
      "Iterated 265000 sentences\n",
      "Iterated 266000 sentences\n",
      "Iterated 267000 sentences\n",
      "Iterated 268000 sentences\n",
      "Iterated 269000 sentences\n",
      "Iterated 270000 sentences\n",
      "Iterated 271000 sentences\n",
      "Iterated 272000 sentences\n",
      "Iterated 273000 sentences\n",
      "Iterated 274000 sentences\n",
      "Iterated 275000 sentences\n",
      "Iterated 276000 sentences\n",
      "Iterated 277000 sentences\n",
      "Iterated 278000 sentences\n",
      "Iterated 279000 sentences\n",
      "Iterated 280000 sentences\n",
      "Iterated 281000 sentences\n",
      "Iterated 282000 sentences\n",
      "Iterated 283000 sentences\n",
      "Iterated 284000 sentences\n",
      "Iterated 285000 sentences\n",
      "Iterated 286000 sentences\n",
      "Iterated 287000 sentences\n",
      "Iterated 288000 sentences\n",
      "Iterated 289000 sentences\n",
      "Iterated 290000 sentences\n",
      "Iterated 291000 sentences\n",
      "Iterated 292000 sentences\n",
      "Iterated 293000 sentences\n",
      "Iterated 294000 sentences\n",
      "Iterated 295000 sentences\n",
      "Iterated 296000 sentences\n",
      "Iterated 297000 sentences\n",
      "Iterated 298000 sentences\n",
      "Iterated 299000 sentences\n",
      "Iterated 300000 sentences\n",
      "Iterated 301000 sentences\n",
      "Iterated 302000 sentences\n",
      "Iterated 303000 sentences\n",
      "Iterated 304000 sentences\n",
      "Iterated 305000 sentences\n",
      "Iterated 306000 sentences\n",
      "Iterated 307000 sentences\n",
      "Iterated 308000 sentences\n",
      "Iterated 309000 sentences\n",
      "Iterated 310000 sentences\n",
      "Iterated 311000 sentences\n",
      "Iterated 312000 sentences\n",
      "Iterated 313000 sentences\n",
      "Iterated 314000 sentences\n",
      "Iterated 315000 sentences\n",
      "Iterated 316000 sentences\n",
      "Iterated 317000 sentences\n",
      "Iterated 318000 sentences\n",
      "Iterated 319000 sentences\n",
      "Iterated 320000 sentences\n",
      "Iterated 321000 sentences\n",
      "Iterated 322000 sentences\n",
      "Iterated 323000 sentences\n",
      "Iterated 324000 sentences\n",
      "Added 8000 words to vocab\n",
      "Iterated 325000 sentences\n",
      "Iterated 326000 sentences\n",
      "Iterated 327000 sentences\n",
      "Iterated 328000 sentences\n",
      "Iterated 329000 sentences\n",
      "Iterated 330000 sentences\n",
      "Iterated 331000 sentences\n",
      "Iterated 332000 sentences\n",
      "Iterated 333000 sentences\n",
      "Iterated 334000 sentences\n",
      "Iterated 335000 sentences\n",
      "Iterated 336000 sentences\n",
      "Iterated 337000 sentences\n",
      "Iterated 338000 sentences\n",
      "Iterated 339000 sentences\n",
      "Iterated 340000 sentences\n",
      "Iterated 341000 sentences\n",
      "Iterated 342000 sentences\n",
      "Iterated 343000 sentences\n",
      "Iterated 344000 sentences\n",
      "Iterated 345000 sentences\n",
      "Iterated 346000 sentences\n",
      "Iterated 347000 sentences\n",
      "Iterated 348000 sentences\n",
      "Iterated 349000 sentences\n",
      "Iterated 350000 sentences\n",
      "Iterated 351000 sentences\n",
      "Iterated 352000 sentences\n",
      "Iterated 353000 sentences\n",
      "Iterated 354000 sentences\n",
      "Iterated 355000 sentences\n",
      "Iterated 356000 sentences\n",
      "Iterated 357000 sentences\n",
      "Iterated 358000 sentences\n",
      "Iterated 359000 sentences\n",
      "Iterated 360000 sentences\n",
      "Iterated 361000 sentences\n",
      "Iterated 362000 sentences\n",
      "Iterated 363000 sentences\n",
      "Iterated 364000 sentences\n",
      "Iterated 365000 sentences\n",
      "Iterated 366000 sentences\n",
      "Iterated 367000 sentences\n",
      "Iterated 368000 sentences\n",
      "Iterated 369000 sentences\n",
      "Iterated 370000 sentences\n",
      "Iterated 371000 sentences\n",
      "Iterated 372000 sentences\n",
      "Iterated 373000 sentences\n",
      "Iterated 374000 sentences\n",
      "Iterated 375000 sentences\n",
      "Iterated 376000 sentences\n",
      "Iterated 377000 sentences\n",
      "Iterated 378000 sentences\n",
      "Iterated 379000 sentences\n",
      "Iterated 380000 sentences\n",
      "Iterated 381000 sentences\n",
      "Iterated 382000 sentences\n",
      "Iterated 383000 sentences\n",
      "Iterated 384000 sentences\n",
      "Iterated 385000 sentences\n",
      "Iterated 386000 sentences\n",
      "Iterated 387000 sentences\n",
      "Iterated 388000 sentences\n",
      "Iterated 389000 sentences\n",
      "Iterated 390000 sentences\n",
      "Iterated 391000 sentences\n",
      "Iterated 392000 sentences\n",
      "Iterated 393000 sentences\n",
      "Iterated 394000 sentences\n",
      "Iterated 395000 sentences\n",
      "Iterated 396000 sentences\n",
      "Iterated 397000 sentences\n",
      "Iterated 398000 sentences\n",
      "Iterated 399000 sentences\n",
      "Iterated 400000 sentences\n",
      "Iterated 401000 sentences\n",
      "Iterated 402000 sentences\n",
      "Iterated 403000 sentences\n",
      "Iterated 404000 sentences\n",
      "Iterated 405000 sentences\n",
      "Iterated 406000 sentences\n",
      "Iterated 407000 sentences\n",
      "Iterated 408000 sentences\n",
      "Iterated 409000 sentences\n",
      "Iterated 410000 sentences\n",
      "Iterated 411000 sentences\n",
      "Iterated 412000 sentences\n",
      "Iterated 413000 sentences\n",
      "Iterated 414000 sentences\n",
      "Iterated 415000 sentences\n",
      "Iterated 416000 sentences\n",
      "Iterated 417000 sentences\n",
      "Iterated 418000 sentences\n",
      "Iterated 419000 sentences\n",
      "Iterated 420000 sentences\n",
      "Iterated 421000 sentences\n",
      "Iterated 422000 sentences\n",
      "Iterated 423000 sentences\n",
      "Iterated 424000 sentences\n",
      "Iterated 425000 sentences\n",
      "Iterated 426000 sentences\n",
      "Iterated 427000 sentences\n",
      "Iterated 428000 sentences\n",
      "Iterated 429000 sentences\n",
      "Iterated 430000 sentences\n",
      "Iterated 431000 sentences\n",
      "Iterated 432000 sentences\n",
      "Iterated 433000 sentences\n",
      "Iterated 434000 sentences\n",
      "Iterated 435000 sentences\n",
      "Iterated 436000 sentences\n",
      "Added 9000 words to vocab\n",
      "Iterated 437000 sentences\n",
      "Iterated 438000 sentences\n",
      "Iterated 439000 sentences\n",
      "Iterated 440000 sentences\n",
      "Iterated 441000 sentences\n",
      "Iterated 442000 sentences\n",
      "Iterated 443000 sentences\n",
      "Iterated 444000 sentences\n",
      "Iterated 445000 sentences\n",
      "Iterated 446000 sentences\n",
      "Iterated 447000 sentences\n",
      "Iterated 448000 sentences\n",
      "Iterated 449000 sentences\n",
      "Iterated 450000 sentences\n",
      "Iterated 451000 sentences\n",
      "Iterated 452000 sentences\n",
      "Iterated 453000 sentences\n",
      "Iterated 454000 sentences\n",
      "Iterated 455000 sentences\n",
      "Iterated 456000 sentences\n",
      "Iterated 457000 sentences\n",
      "Iterated 458000 sentences\n",
      "Iterated 459000 sentences\n",
      "Iterated 460000 sentences\n",
      "Iterated 461000 sentences\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterated 462000 sentences\n",
      "Iterated 463000 sentences\n",
      "Iterated 464000 sentences\n",
      "Iterated 465000 sentences\n",
      "Iterated 466000 sentences\n",
      "Iterated 467000 sentences\n",
      "Iterated 468000 sentences\n",
      "Iterated 469000 sentences\n",
      "Iterated 470000 sentences\n",
      "Iterated 471000 sentences\n",
      "Iterated 472000 sentences\n",
      "Iterated 473000 sentences\n",
      "Iterated 474000 sentences\n",
      "Iterated 475000 sentences\n",
      "Iterated 476000 sentences\n",
      "Iterated 477000 sentences\n",
      "Iterated 478000 sentences\n",
      "Iterated 479000 sentences\n",
      "Iterated 480000 sentences\n",
      "Iterated 481000 sentences\n",
      "Iterated 482000 sentences\n",
      "Iterated 483000 sentences\n",
      "Iterated 484000 sentences\n",
      "Iterated 485000 sentences\n",
      "Iterated 486000 sentences\n",
      "Iterated 487000 sentences\n",
      "Iterated 488000 sentences\n",
      "Iterated 489000 sentences\n",
      "Iterated 490000 sentences\n",
      "Iterated 491000 sentences\n",
      "Iterated 492000 sentences\n",
      "Iterated 493000 sentences\n",
      "Iterated 494000 sentences\n",
      "Iterated 495000 sentences\n",
      "Iterated 496000 sentences\n",
      "Iterated 497000 sentences\n",
      "Iterated 498000 sentences\n",
      "Iterated 499000 sentences\n",
      "Iterated 500000 sentences\n",
      "Iterated 501000 sentences\n",
      "Iterated 502000 sentences\n",
      "Iterated 503000 sentences\n",
      "Iterated 504000 sentences\n",
      "Iterated 505000 sentences\n",
      "Iterated 506000 sentences\n",
      "Iterated 507000 sentences\n",
      "Iterated 508000 sentences\n",
      "Iterated 509000 sentences\n",
      "Iterated 510000 sentences\n",
      "Iterated 511000 sentences\n",
      "Iterated 512000 sentences\n",
      "Iterated 513000 sentences\n",
      "Iterated 514000 sentences\n",
      "Iterated 515000 sentences\n",
      "Iterated 516000 sentences\n",
      "Iterated 517000 sentences\n",
      "Iterated 518000 sentences\n",
      "Iterated 519000 sentences\n",
      "Iterated 520000 sentences\n",
      "Iterated 521000 sentences\n",
      "Iterated 522000 sentences\n",
      "Iterated 523000 sentences\n",
      "Iterated 524000 sentences\n",
      "Iterated 525000 sentences\n",
      "Iterated 526000 sentences\n",
      "Iterated 527000 sentences\n",
      "Iterated 528000 sentences\n",
      "Iterated 529000 sentences\n",
      "Iterated 530000 sentences\n",
      "Iterated 531000 sentences\n",
      "Iterated 532000 sentences\n",
      "Iterated 533000 sentences\n",
      "Iterated 534000 sentences\n",
      "Iterated 535000 sentences\n",
      "Iterated 536000 sentences\n",
      "Iterated 537000 sentences\n",
      "Iterated 538000 sentences\n",
      "Iterated 539000 sentences\n",
      "Iterated 540000 sentences\n",
      "Iterated 541000 sentences\n",
      "Iterated 542000 sentences\n",
      "Iterated 543000 sentences\n",
      "Iterated 544000 sentences\n",
      "Iterated 545000 sentences\n",
      "Iterated 546000 sentences\n",
      "Iterated 547000 sentences\n",
      "Iterated 548000 sentences\n",
      "Iterated 549000 sentences\n",
      "Iterated 550000 sentences\n",
      "Iterated 551000 sentences\n",
      "Iterated 552000 sentences\n",
      "Iterated 553000 sentences\n",
      "Iterated 554000 sentences\n",
      "Iterated 555000 sentences\n",
      "Iterated 556000 sentences\n",
      "Iterated 557000 sentences\n",
      "Iterated 558000 sentences\n",
      "Iterated 559000 sentences\n",
      "Iterated 560000 sentences\n",
      "Iterated 561000 sentences\n",
      "Iterated 562000 sentences\n",
      "Iterated 563000 sentences\n",
      "Iterated 564000 sentences\n",
      "Iterated 565000 sentences\n",
      "Added 10000 words to vocab\n",
      "Iterated 566000 sentences\n",
      "Iterated 567000 sentences\n",
      "Iterated 568000 sentences\n",
      "Iterated 569000 sentences\n",
      "Iterated 570000 sentences\n",
      "Iterated 571000 sentences\n",
      "Iterated 572000 sentences\n",
      "Iterated 573000 sentences\n",
      "Iterated 574000 sentences\n",
      "Iterated 575000 sentences\n",
      "Iterated 576000 sentences\n",
      "Iterated 577000 sentences\n",
      "Iterated 578000 sentences\n",
      "Iterated 579000 sentences\n",
      "Iterated 580000 sentences\n",
      "Iterated 581000 sentences\n",
      "Iterated 582000 sentences\n",
      "Iterated 583000 sentences\n",
      "Iterated 584000 sentences\n",
      "Iterated 585000 sentences\n",
      "Iterated 586000 sentences\n",
      "Iterated 587000 sentences\n",
      "Iterated 588000 sentences\n",
      "Iterated 589000 sentences\n",
      "Iterated 590000 sentences\n",
      "Iterated 591000 sentences\n",
      "Done, added 10217 words to vocabulary\n",
      "Finished building vocabulary\n",
      "Using 10218 words\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset =  COCODataset(\n",
    "    root_dir = \"/home/yandex/DLW2021/davidhay/Deep-learning-image-caption/coco/images/train2017\",\n",
    "    annotation_file= \"/home/yandex/DLW2021/davidhay/Deep-learning-image-caption/coco/annotations/captions_train2017.json\",\n",
    "    transform=transformation,\n",
    "    freq_threshold=5,\n",
    "    load_vocab=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pa_fIXj5_n6W"
   },
   "outputs": [],
   "source": [
    "print(dataset.vocab.stoi) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "e1ptid0sedWA"
   },
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 1\n",
    "NUM_WORKER = 1\n",
    "#token to represent the padding\n",
    "pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKER,\n",
    "    shuffle=False,\n",
    "    collate_fn=CapsCollate(pad_idx=pad_idx,batch_first=True, vec_len=75)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXIFRNd6Rbih"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "kxeR_Dg0JnNQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "  def __init__(self, embed_size, train_CNN=False):\n",
    "      super(EncoderCNN, self).__init__()\n",
    "      self.train_CNN = train_CNN\n",
    "      self.inception = models.inception_v3(pretrained=True, aux_logits=False)\n",
    "      self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n",
    "      self.relu = nn.ReLU()\n",
    "      self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "  def forward(self, images):\n",
    "      \n",
    "      features = self.inception(images)\n",
    "      output = self.dropout(self.relu(features))\n",
    "      return output\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "  \"\"\"\n",
    "  Input is a CNN network, output will be a caption.\n",
    "  TODO: Check how to implement a transformer for better results\n",
    "  \"\"\"\n",
    "  def __init__(self, embed_size, hidden_size, vocab_size):\n",
    "      super(DecoderRNN, self).__init__()\n",
    "      self.hidden_size = hidden_size\n",
    "      self.vocab_size = vocab_size\n",
    "      self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "      self.lstm_cell = nn.LSTMCell(embed_size, hidden_size)\n",
    "      self.fc_out = nn.Linear(hidden_size, vocab_size)\n",
    "      self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "  def forward(self, features, captions, show=False):\n",
    "        # batch size\n",
    "        batch_size = features.size(0)\n",
    "        \n",
    "        \n",
    "        # init the hidden and cell states to zeros\n",
    "        hidden_state = torch.zeros((batch_size, self.hidden_size)).to(device)\n",
    "        cell_state = torch.zeros((batch_size, self.hidden_size)).to(device)\n",
    "        hidden_state, cell_state = self.lstm_cell(features, (hidden_state, cell_state))\n",
    "        # define the output tensor placeholder\n",
    "        outputs = torch.empty((batch_size, captions.size(1), self.vocab_size)).to(device)\n",
    "\n",
    "        # embed the captions\n",
    "        captions_embed = self.embed(captions)\n",
    "        # tensor of shape (B, LEN, EMBED SIZE)\n",
    "        # LEN- vectors length (longest caption+2)\n",
    "        \n",
    "        # pass the caption word by word\n",
    "        for t in range(captions.size(1)):\n",
    "\n",
    "            # for the first time step the input is the feature vector\n",
    "            # if t == 0:\n",
    "            #     hidden_state, cell_state = self.lstm_cell(features, (hidden_state, cell_state))\n",
    "                \n",
    "            # # for the 2nd+ time step, using teacher forcer\n",
    "            # else:\n",
    "            hidden_state, cell_state = self.lstm_cell(captions_embed[:, t, :], (hidden_state, cell_state))\n",
    "            # output of the attention mechanism\n",
    "            out = self.fc_out(self.dropout(hidden_state))\n",
    "            # build the output tensor\n",
    "            outputs[:, t, :] = out\n",
    "        if show:\n",
    "            #print(f\"Captions:{captions}\")\n",
    "            #print(f\"outputs shape:{outputs.shape}\")\n",
    "            pass\n",
    "        return outputs\n",
    "\n",
    "  \n",
    "\n",
    "class CNNtoRNN(nn.Module):\n",
    "  def __init__(self, embed_size, hidden_size, vocab_size, train_CNN=False):\n",
    "      super(CNNtoRNN, self).__init__()\n",
    "      self.encoderCNN = EncoderCNN(embed_size, train_CNN).to(device)\n",
    "      self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size).to(device)\n",
    "      #self.decoderRNN = DecoderRNNConcat(embed_size, hidden_size, vocab_size).to(device)\n",
    "\n",
    "  def forward(self, images, captions, show=False):\n",
    "      features = self.encoderCNN(images)\n",
    "      outputs = self.decoderRNN(features, captions, show)\n",
    "      return outputs\n",
    "  def caption_images(self, image, vocab, max_len=50):\n",
    "      # Inference part\n",
    "      # Given the image features generate the captions\n",
    "      # input shape: (3,x,y) where, x,y: image size\n",
    "      # ouput: captions list\n",
    "    batch_size = image.size(0)\n",
    "    assert batch_size == 1, \"Caption 1 image at a time\"\n",
    "    image_pred = self.encoderCNN(image)\n",
    "\n",
    "    # init the hidden and cell states to zeros\n",
    "    hidden_state = torch.zeros((1, self.decoderRNN.hidden_size)).to(device)\n",
    "    cell_state = torch.zeros((1, self.decoderRNN.hidden_size)).to(device)\n",
    "    \n",
    "    #starting input is \n",
    "    captions = list()\n",
    "    outputs = torch.empty((batch_size, max_len, self.decoderRNN.vocab_size)).to(device)\n",
    "    hidden_state, cell_state = self.decoderRNN.lstm_cell(image_pred, (hidden_state, cell_state))\n",
    "    out = self.decoderRNN.fc_out(self.decoderRNN.dropout(hidden_state))\n",
    "    word_embed = self.decoderRNN.embed(torch.tensor(vocab.stoi[\"<SOS>\"]).to(device)).unsqueeze(0)\n",
    "    for t in range(max_len):\n",
    "        # for the first time step the input is the feature vector\n",
    "        # if t == 0:\n",
    "        #     hidden_state, cell_state = self.decoderRNN.lstm_cell(image_pred, (hidden_state, cell_state))\n",
    "        # for the 2nd+ time step, use previously generated caption\n",
    "        # else:\n",
    "        hidden_state, cell_state = self.decoderRNN.lstm_cell(word_embed, (hidden_state, cell_state))\n",
    "        \n",
    "        # output of the attention mechanism\n",
    "        out = self.decoderRNN.fc_out(hidden_state)\n",
    "        outputs[:, t, :] = out\n",
    "        #print(f\"out shape:{out.shape}\")\n",
    "        captions.append(torch.argmax(out,dim=1))\n",
    "        #print(f\"\\n predicted outputs:{captions}\")\n",
    "        word_embed = self.decoderRNN.embed(torch.argmax(out[0])).unsqueeze(0)\n",
    "        last_word_idx = captions[-1].item()\n",
    "        if vocab.itos[last_word_idx] == vocab.stoi[\"<EOS>\"]:\n",
    "            print(\"BREAKING!!!!!!\")\n",
    "            break\n",
    "            \n",
    "            \n",
    "            # build the output tensor\n",
    "        #print(captions)\n",
    "        #covert the vocab idx to words and return sentence\n",
    "    print(f\"outputs shape:{outputs.shape}\")\n",
    "    print(f\"Outputs argmax dim2:{torch.argmax(outputs,dim=2)}\")\n",
    "    return [vocab.itos[idx.item()] for idx in captions if idx.item() != vocab.stoi[\"<PAD>\"]]\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTBnd6f5RgQd"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qTIQN7TWqOJ"
   },
   "source": [
    "## training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Yu_HfkQyRiAP"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(max_epochs, model):\n",
    "  # Hyperparameters\n",
    "  learning_rate = 3e-4\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "  \n",
    "  # init model\n",
    "  model = model.to(device)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "  model.train()\n",
    "     \n",
    "\n",
    "  # start epochs\n",
    "  for epoch in range(max_epochs):\n",
    "    for idx, (img, captions) in tqdm(\n",
    "            enumerate(data_loader), total=len(data_loader), leave=False\n",
    "        ):\n",
    "      img = img.to(device)\n",
    "      captions = captions.to(device).long()\n",
    "      output = model(img, captions).to(device)\n",
    "      loss = criterion(output.reshape(-1, output.shape[2]), captions.reshape(-1))\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward(loss)\n",
    "      optimizer.step()\n",
    "\n",
    "      if idx>0 and idx%100==0:\n",
    "        dataiter = iter(data_loader)\n",
    "        img_show,cap = next(dataiter)\n",
    "        print(f\"Loss {loss.item():.5f}\\n\")\n",
    "        demo_cap = model.caption_images(img_show[0:1].to(device), vocab=dataset.vocab, max_len=30)\n",
    "        demo_cap = ' '.join(demo_cap)\n",
    "        print(\"Predicted\")\n",
    "        show_image(img_show[0],title=demo_cap)\n",
    "        print(\"Original\")\n",
    "        cap = cap[0]\n",
    "        print(cap.long())\n",
    "        demo_cap = ' '.join([dataset.vocab.itos[idx2.item()] for idx2 in cap if idx2.item() != dataset.vocab.stoi[\"<PAD>\"]])\n",
    "        show_image(img_show[0],title=demo_cap, transform=False)\n",
    "        \n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CWQGhuOWvF9"
   },
   "source": [
    "## image function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "a1APdbNyWw8R"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_image(img, title=None, transform=True):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    \n",
    "    #unnormalize \n",
    "    if transform:\n",
    "      img[0] = img[0] * 0.229\n",
    "      img[1] = img[1] * 0.224 \n",
    "      img[2] = img[2] * 0.225 \n",
    "      img[0] += 0.485 \n",
    "      img[1] += 0.456 \n",
    "      img[2] += 0.406\n",
    "      \n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    \n",
    "    plt.imshow(img)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KceQyQ0uqiRM"
   },
   "source": [
    "## Overfit sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "JcQJ733Nql1V"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "tqdm = partial(tqdm, position=0, leave=True)\n",
    "def overfit(model, T=250):\n",
    "    learning_rate = 3e-4\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    \n",
    "    # init model\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "\n",
    "    \n",
    "    dataiter = iter(data_loader)\n",
    "    img,caption = next(dataiter)\n",
    "    for i in tqdm(range(T)):\n",
    "        # train on the same image and caption to achieve overfitting\n",
    "        img = img.to(device)\n",
    "        caption = caption.to(device).long()\n",
    "        output = model(img, caption, show=False).to(device)\n",
    "        loss = criterion(output.reshape(-1, output.shape[2]), caption.reshape(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(loss)\n",
    "        optimizer.step()\n",
    "    output = model(img, caption, show=False).to(device)\n",
    "    show_img = img.to(\"cpu\")\n",
    "    print(f\"\\n\\nLoss {loss.item():.5f}\\n\")\n",
    "    #print(f\"\\nForward\\n\")\n",
    "    out_cap = torch.argmax(output[0],dim=1)\n",
    "    #print(f\"Forwad num vals:{out_cap}\")\n",
    "    demo_cap = ' '.join([dataset.vocab.itos[idx2.item()] for idx2 in out_cap if idx2.item() != dataset.vocab.stoi[\"<PAD>\"]])\n",
    "    show_image(show_img[0],title=demo_cap)\n",
    "    print(\"Predicted\")\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        demo_cap = model.caption_images(show_img[0:1].to(device), vocab=dataset.vocab, max_len=15)\n",
    "        demo_cap = ' '.join(demo_cap)\n",
    "        model.train()\n",
    "        \n",
    "        show_image(show_img[0],title=demo_cap, transform=False)\n",
    "    print(\"Original\")\n",
    "    cap = caption[0]\n",
    "    #print(cap.long())\n",
    "    demo_cap = ' '.join([dataset.vocab.itos[idx2.item()] for idx2 in cap if idx2.item() != dataset.vocab.stoi[\"<PAD>\"]])\n",
    "    show_image(show_img[0],title=demo_cap, transform=False)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rABCvI85Wx_9"
   },
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6aG4SBFV7UM"
   },
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "hidden_size = 5000\n",
    "vocab_size = len(dataset.vocab)\n",
    "model = CNNtoRNN(embed_size, hidden_size, vocab_size, train_CNN=False)\n",
    "trained_model = train(3, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Eqf6B56tjS1"
   },
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "QQmoP_Ltd9pM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /home_dir/.cache/torch/hub/checkpoints/inception_v3_google-1a9a5a14.pth\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-4928b62b6a7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNNtoRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_CNN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0moverfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-9871efbb3806>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embed_size, hidden_size, vocab_size, train_CNN)\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_CNN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m       \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCNNtoRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoderCNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_CNN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoderRNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m       \u001b[0;31m#self.decoderRNN = DecoderRNNConcat(embed_size, hidden_size, vocab_size).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-9871efbb3806>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embed_size, train_CNN)\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEncoderCNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_CNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_CNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minception_v3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36minception_v3\u001b[0;34m(pretrained, progress, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'init_weights'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# we are loading weights from a pretrained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInception3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         state_dict = load_state_dict_from_url(model_urls['inception_v3_google'],\n\u001b[0m\u001b[1;32m     54\u001b[0m                                               progress=progress)\n\u001b[1;32m     55\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/hub.py\u001b[0m in \u001b[0;36mload_state_dict_from_url\u001b[0;34m(url, model_dir, map_location, progress, check_hash, file_name)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHASH_REGEX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# r is Optional[Match[str]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0mhash_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m         \u001b[0mdownload_url_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_legacy_zip_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/hub.py\u001b[0m in \u001b[0;36mdownload_url_to_file\u001b[0;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhash_prefix\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0msha256\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msha256\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         with tqdm(total=file_size, disable=not progress,\n\u001b[0m\u001b[1;32m    411\u001b[0m                   unit='B', unit_scale=True, unit_divisor=1024) as pbar:\n\u001b[1;32m    412\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0munit_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0munit_scale\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         self.container = self.status_printer(\n\u001b[0m\u001b[1;32m    232\u001b[0m             self.fp, total, self.desc, self.ncols)\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdisplay_here\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# Prepare IPython progress bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mIProgress\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# #187 #451 #558 #872\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0;34m\"IProgress not found. Please update jupyter and ipywidgets.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;34m\" See https://ipywidgets.readthedocs.io/en/stable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "embed_size = 400\n",
    "hidden_size = 3000\n",
    "vocab_size = len(dataset.vocab)\n",
    "model = CNNtoRNN(embed_size, hidden_size, vocab_size, train_CNN=False)\n",
    "overfit(model,250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WtuAGKC3gftz"
   },
   "outputs": [],
   "source": [
    "for imgs,caps in data_loader:\n",
    "    print(f\"\\ncap shape:{caps.shape}\")\n",
    "    print(f\"\\nFirst caption:{caps[0]}\")\n",
    "    print(f\"\\nFirst embed:{caps[0][0]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.9.2\n",
      "  latest version: 4.10.3\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - ipywidgets\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    argon2-cffi-20.1.0         |   py38h25fe258_2          47 KB  conda-forge\n",
      "    async_generator-1.10       |             py_0          18 KB  conda-forge\n",
      "    bleach-4.0.0               |     pyhd8ed1ab_0         111 KB  conda-forge\n",
      "    ca-certificates-2021.5.30  |       ha878542_0         136 KB  conda-forge\n",
      "    certifi-2021.5.30          |   py38h578d9bd_0         141 KB  conda-forge\n",
      "    conda-4.10.3               |   py38h578d9bd_0         3.1 MB  conda-forge\n",
      "    defusedxml-0.7.1           |     pyhd8ed1ab_0          23 KB  conda-forge\n",
      "    entrypoints-0.3            |  pyhd8ed1ab_1003           8 KB  conda-forge\n",
      "    ipykernel-5.5.5            |   py38hd0cf306_0         167 KB  conda-forge\n",
      "    ipywidgets-7.6.3           |     pyhd3deb0d_0         101 KB  conda-forge\n",
      "    jupyter_client-6.1.12      |     pyhd8ed1ab_0          79 KB  conda-forge\n",
      "    jupyter_core-4.7.1         |   py38h578d9bd_0          72 KB  conda-forge\n",
      "    jupyterlab_pygments-0.1.2  |     pyh9f0ad1d_0           8 KB  conda-forge\n",
      "    jupyterlab_widgets-1.0.0   |     pyhd8ed1ab_1         130 KB  conda-forge\n",
      "    libsodium-1.0.18           |       h36c2ea0_1         366 KB  conda-forge\n",
      "    mistune-0.8.4              |py38h25fe258_1002          54 KB  conda-forge\n",
      "    nbclient-0.5.4             |     pyhd8ed1ab_0          60 KB  conda-forge\n",
      "    nbconvert-6.1.0            |   py38h578d9bd_0         521 KB  conda-forge\n",
      "    nbformat-5.1.3             |     pyhd8ed1ab_0          47 KB  conda-forge\n",
      "    nest-asyncio-1.5.1         |     pyhd8ed1ab_0           9 KB  conda-forge\n",
      "    notebook-6.4.3             |     pyha770c72_0         6.3 MB  conda-forge\n",
      "    openssl-1.1.1k             |       h27cfd23_0         2.5 MB\n",
      "    packaging-21.0             |     pyhd8ed1ab_0          35 KB  conda-forge\n",
      "    pandoc-2.14.1              |       h7f98852_0        12.0 MB  conda-forge\n",
      "    pandocfilters-1.4.2        |             py_1           9 KB  conda-forge\n",
      "    prometheus_client-0.11.0   |     pyhd8ed1ab_0          46 KB  conda-forge\n",
      "    pyparsing-2.4.7            |     pyh9f0ad1d_0          60 KB  conda-forge\n",
      "    python-dateutil-2.8.2      |     pyhd8ed1ab_0         240 KB  conda-forge\n",
      "    python_abi-3.8             |           2_cp38           4 KB  conda-forge\n",
      "    pyzmq-19.0.2               |   py38ha71036d_2         511 KB  conda-forge\n",
      "    send2trash-1.8.0           |     pyhd8ed1ab_0          17 KB  conda-forge\n",
      "    terminado-0.11.1           |   py38h578d9bd_0          27 KB  conda-forge\n",
      "    testpath-0.5.0             |     pyhd8ed1ab_0          86 KB  conda-forge\n",
      "    tornado-6.1                |   py38h25fe258_0         646 KB  conda-forge\n",
      "    webencodings-0.5.1         |             py_1          12 KB  conda-forge\n",
      "    widgetsnbextension-3.5.1   |   py38h578d9bd_4         1.8 MB  conda-forge\n",
      "    zeromq-4.3.4               |       h2531618_0         331 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        29.8 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  argon2-cffi        conda-forge/linux-64::argon2-cffi-20.1.0-py38h25fe258_2\n",
      "  async_generator    conda-forge/noarch::async_generator-1.10-py_0\n",
      "  bleach             conda-forge/noarch::bleach-4.0.0-pyhd8ed1ab_0\n",
      "  defusedxml         conda-forge/noarch::defusedxml-0.7.1-pyhd8ed1ab_0\n",
      "  entrypoints        conda-forge/noarch::entrypoints-0.3-pyhd8ed1ab_1003\n",
      "  ipykernel          conda-forge/linux-64::ipykernel-5.5.5-py38hd0cf306_0\n",
      "  ipywidgets         conda-forge/noarch::ipywidgets-7.6.3-pyhd3deb0d_0\n",
      "  jupyter_client     conda-forge/noarch::jupyter_client-6.1.12-pyhd8ed1ab_0\n",
      "  jupyter_core       conda-forge/linux-64::jupyter_core-4.7.1-py38h578d9bd_0\n",
      "  jupyterlab_pygmen~ conda-forge/noarch::jupyterlab_pygments-0.1.2-pyh9f0ad1d_0\n",
      "  jupyterlab_widgets conda-forge/noarch::jupyterlab_widgets-1.0.0-pyhd8ed1ab_1\n",
      "  libsodium          conda-forge/linux-64::libsodium-1.0.18-h36c2ea0_1\n",
      "  mistune            conda-forge/linux-64::mistune-0.8.4-py38h25fe258_1002\n",
      "  nbclient           conda-forge/noarch::nbclient-0.5.4-pyhd8ed1ab_0\n",
      "  nbconvert          conda-forge/linux-64::nbconvert-6.1.0-py38h578d9bd_0\n",
      "  nbformat           conda-forge/noarch::nbformat-5.1.3-pyhd8ed1ab_0\n",
      "  nest-asyncio       conda-forge/noarch::nest-asyncio-1.5.1-pyhd8ed1ab_0\n",
      "  notebook           conda-forge/noarch::notebook-6.4.3-pyha770c72_0\n",
      "  packaging          conda-forge/noarch::packaging-21.0-pyhd8ed1ab_0\n",
      "  pandoc             conda-forge/linux-64::pandoc-2.14.1-h7f98852_0\n",
      "  pandocfilters      conda-forge/noarch::pandocfilters-1.4.2-py_1\n",
      "  prometheus_client  conda-forge/noarch::prometheus_client-0.11.0-pyhd8ed1ab_0\n",
      "  pyparsing          conda-forge/noarch::pyparsing-2.4.7-pyh9f0ad1d_0\n",
      "  python-dateutil    conda-forge/noarch::python-dateutil-2.8.2-pyhd8ed1ab_0\n",
      "  python_abi         conda-forge/linux-64::python_abi-3.8-2_cp38\n",
      "  pyzmq              conda-forge/linux-64::pyzmq-19.0.2-py38ha71036d_2\n",
      "  send2trash         conda-forge/noarch::send2trash-1.8.0-pyhd8ed1ab_0\n",
      "  terminado          conda-forge/linux-64::terminado-0.11.1-py38h578d9bd_0\n",
      "  testpath           conda-forge/noarch::testpath-0.5.0-pyhd8ed1ab_0\n",
      "  tornado            conda-forge/linux-64::tornado-6.1-py38h25fe258_0\n",
      "  webencodings       conda-forge/noarch::webencodings-0.5.1-py_1\n",
      "  widgetsnbextension conda-forge/linux-64::widgetsnbextension-3.5.1-py38h578d9bd_4\n",
      "  zeromq             pkgs/main/linux-64::zeromq-4.3.4-h2531618_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2020.10.14~ --> conda-forge::ca-certificates-2021.5.30-ha878542_0\n",
      "  certifi            pkgs/main::certifi-2020.11.8-py38h06a~ --> conda-forge::certifi-2021.5.30-py38h578d9bd_0\n",
      "  conda               pkgs/main::conda-4.9.2-py38h06a4308_0 --> conda-forge::conda-4.10.3-py38h578d9bd_0\n",
      "  openssl                                 1.1.1h-h7b6447c_0 --> 1.1.1k-h27cfd23_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "python-dateutil-2.8. | 240 KB    | ##################################### | 100% \n",
      "pandoc-2.14.1        | 12.0 MB   | ##################################### | 100% \n",
      "packaging-21.0       | 35 KB     | ##################################### | 100% \n",
      "jupyter_client-6.1.1 | 79 KB     | ##################################### | 100% \n",
      "ipywidgets-7.6.3     | 101 KB    | ##################################### | 100% \n",
      "notebook-6.4.3       | 6.3 MB    | ##################################### | 100% \n",
      "ca-certificates-2021 | 136 KB    | ##################################### | 100% \n",
      "nest-asyncio-1.5.1   | 9 KB      | ##################################### | 100% \n",
      "nbclient-0.5.4       | 60 KB     | ##################################### | 100% \n",
      "jupyterlab_widgets-1 | 130 KB    | ##################################### | 100% \n",
      "webencodings-0.5.1   | 12 KB     | ##################################### | 100% \n",
      "defusedxml-0.7.1     | 23 KB     | ##################################### | 100% \n",
      "nbformat-5.1.3       | 47 KB     | ##################################### | 100% \n",
      "entrypoints-0.3      | 8 KB      | ##################################### | 100% \n",
      "python_abi-3.8       | 4 KB      | ##################################### | 100% \n",
      "jupyter_core-4.7.1   | 72 KB     | ##################################### | 100% \n",
      "certifi-2021.5.30    | 141 KB    | ##################################### | 100% \n",
      "widgetsnbextension-3 | 1.8 MB    | ##################################### | 100% \n",
      "ipykernel-5.5.5      | 167 KB    | ##################################### | 100% \n",
      "openssl-1.1.1k       | 2.5 MB    | ##################################### | 100% \n",
      "conda-4.10.3         | 3.1 MB    | ##################################### | 100% \n",
      "libsodium-1.0.18     | 366 KB    | ##################################### | 100% \n",
      "mistune-0.8.4        | 54 KB     | ##################################### | 100% \n",
      "zeromq-4.3.4         | 331 KB    | ##################################### | 100% \n",
      "bleach-4.0.0         | 111 KB    | ##################################### | 100% \n",
      "pyparsing-2.4.7      | 60 KB     | ##################################### | 100% \n",
      "send2trash-1.8.0     | 17 KB     | ##################################### | 100% \n",
      "pandocfilters-1.4.2  | 9 KB      | ##################################### | 100% \n",
      "terminado-0.11.1     | 27 KB     | ##################################### | 100% \n",
      "prometheus_client-0. | 46 KB     | ##################################### | 100% \n",
      "jupyterlab_pygments- | 8 KB      | ##################################### | 100% \n",
      "testpath-0.5.0       | 86 KB     | ##################################### | 100% \n",
      "nbconvert-6.1.0      | 521 KB    | ##################################### | 100% \n",
      "tornado-6.1          | 646 KB    | ##################################### | 100% \n",
      "async_generator-1.10 | 18 KB     | ##################################### | 100% \n",
      "pyzmq-19.0.2         | 511 KB    | ##################################### | 100% \n",
      "argon2-cffi-20.1.0   | 47 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: failed\n",
      "\n",
      "EnvironmentNotWritableError: The current user does not have write permissions to the target environment.\n",
      "  environment location: /opt/conda\n",
      "  uid: 85461\n",
      "  gid: 1000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge -y ipywidgets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CONDA_ROOT=/home/yandex/DLW2021/davidhay/anaconda3: Command not found.\n",
      "/opt/conda/bin/.: Permission denied.\n",
      "return: Command not found.\n",
      "Illegal variable name.\n",
      "\n",
      "     active environment : None\n",
      "            shell level : 0\n",
      "       user config file : /home_dir/.condarc\n",
      " populated config files : \n",
      "          conda version : 4.9.2\n",
      "    conda-build version : 3.20.5\n",
      "         python version : 3.8.5.final.0\n",
      "       virtual packages : __cuda=11.2=0\n",
      "                          __glibc=2.31=0\n",
      "                          __unix=0=0\n",
      "                          __archspec=1=x86_64\n",
      "       base environment : /opt/conda  (read only)\n",
      "           channel URLs : https://repo.anaconda.com/pkgs/main/linux-64\n",
      "                          https://repo.anaconda.com/pkgs/main/noarch\n",
      "                          https://repo.anaconda.com/pkgs/r/linux-64\n",
      "                          https://repo.anaconda.com/pkgs/r/noarch\n",
      "          package cache : /opt/conda/pkgs\n",
      "                          /home_dir/.conda/pkgs\n",
      "       envs directories : /home_dir/.conda/envs\n",
      "                          /opt/conda/envs\n",
      "               platform : linux-64\n",
      "             user-agent : conda/4.9.2 requests/2.24.0 CPython/3.8.5 Linux/4.15.0-65-generic ubuntu/20.04.1 glibc/2.31\n",
      "                UID:GID : 85461:1000\n",
      "             netrc file : None\n",
      "           offline mode : False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!source /home/yandex/DLW2021/davidhay/anaconda3/bin/activate\n",
    "!conda info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\r\n",
      "#\r\n",
      "                         /home/yandex/DLW2021/davidhay/anaconda3\r\n",
      "                         /home/yandex/DLW2021/davidhay/anaconda3/envs/new-env\r\n",
      "base                  *  /opt/conda\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!conda activate /home/yandex/DLW2021/davidhay/anaconda3/envs/new-env\n",
    "!conda info --envs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\r\n",
      "#\r\n",
      "base                  *  /home/yandex/DLW2021/davidhay/anaconda3\r\n",
      "new-env                  /home/yandex/DLW2021/davidhay/anaconda3/envs/new-env\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!conda info --envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - ipykernel\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2021.7.5   |       h06a4308_1         113 KB\n",
      "    certifi-2021.5.30          |   py38h06a4308_0         138 KB\n",
      "    conda-4.10.3               |   py38h06a4308_0         2.9 MB\n",
      "    ipykernel-5.3.4            |   py38h5ca1d4c_0         183 KB\n",
      "    jupyter_client-6.1.12      |     pyhd3eb1b0_0          88 KB\n",
      "    jupyter_core-4.7.1         |   py38h06a4308_0          68 KB\n",
      "    libsodium-1.0.18           |       h7b6447c_0         244 KB\n",
      "    python-dateutil-2.8.2      |     pyhd3eb1b0_0         233 KB\n",
      "    pyzmq-22.2.1               |   py38h295c915_1         465 KB\n",
      "    tornado-6.1                |   py38h27cfd23_0         588 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         4.9 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  ipykernel          pkgs/main/linux-64::ipykernel-5.3.4-py38h5ca1d4c_0\n",
      "  jupyter_client     pkgs/main/noarch::jupyter_client-6.1.12-pyhd3eb1b0_0\n",
      "  jupyter_core       pkgs/main/linux-64::jupyter_core-4.7.1-py38h06a4308_0\n",
      "  libsodium          pkgs/main/linux-64::libsodium-1.0.18-h7b6447c_0\n",
      "  python-dateutil    pkgs/main/noarch::python-dateutil-2.8.2-pyhd3eb1b0_0\n",
      "  pyzmq              pkgs/main/linux-64::pyzmq-22.2.1-py38h295c915_1\n",
      "  tornado            pkgs/main/linux-64::tornado-6.1-py38h27cfd23_0\n",
      "  zeromq             pkgs/main/linux-64::zeromq-4.3.4-h2531618_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                              2020.10.14-0 --> 2021.7.5-h06a4308_1\n",
      "  certifi                          2020.11.8-py38h06a4308_0 --> 2021.5.30-py38h06a4308_0\n",
      "  conda                                4.9.2-py38h06a4308_0 --> 4.10.3-py38h06a4308_0\n",
      "  openssl                                 1.1.1h-h7b6447c_0 --> 1.1.1k-h27cfd23_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "jupyter_client-6.1.1 | 88 KB     | ##################################### | 100% \n",
      "jupyter_core-4.7.1   | 68 KB     | ##################################### | 100% \n",
      "ca-certificates-2021 | 113 KB    | ##################################### | 100% \n",
      "tornado-6.1          | 588 KB    | ##################################### | 100% \n",
      "python-dateutil-2.8. | 233 KB    | ##################################### | 100% \n",
      "libsodium-1.0.18     | 244 KB    | ##################################### | 100% \n",
      "pyzmq-22.2.1         | 465 KB    | ##################################### | 100% \n",
      "ipykernel-5.3.4      | 183 KB    | ##################################### | 100% \n",
      "conda-4.10.3         | 2.9 MB    | ##################################### | 100% \n",
      "certifi-2021.5.30    | 138 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: failed\n",
      "\n",
      "EnvironmentNotWritableError: The current user does not have write permissions to the target environment.\n",
      "  environment location: /opt/conda\n",
      "  uid: 85461\n",
      "  gid: 1000\n",
      "\n",
      "\n",
      "python: can't open file 'kernel': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!conda install -y ipykernel\n",
    "!python kernel install --user --name=new-env"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "image captioning deep learning workshop.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
