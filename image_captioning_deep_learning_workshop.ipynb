{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Downloads"
   ],
   "metadata": {
    "id": "PJ_DSjOA83Oe"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#%cd /home/yandex/DLW2021/davidhay/\n",
    "#!pwd\n",
    "#!ls -l "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#!rm -rf coco\n",
    "#!mkdir coco\n",
    "#%cd coco"
   ],
   "outputs": [],
   "metadata": {
    "id": "IkLieRxMxqdZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#!wget -c http://images.cocodataset.org/zips/train2017.zip\n",
    "#!unzip train2017.zip\n",
    "#!rm train2017.zip\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#!wget -c http://images.cocodataset.org/zips/val2017.zip\n",
    "\n",
    "#!unzip val2017.zip\n",
    "\n",
    "#!rm val2017.zip\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\n",
    "#!wget -c http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "\n",
    "#!unzip annotations_trainval2017.zip\n",
    "\n",
    "#!rm annotations_trainval2017.zip"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Loading"
   ],
   "metadata": {
    "id": "FEZMYzlIxCwV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports and Vocabulary "
   ],
   "metadata": {
    "id": "pnc6rR2P-ck-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "\n",
    "import os\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self,freq_threshold):\n",
    "        #setting the pre-reserved tokens int to string tokens\n",
    "        # PAD- padding symbol\n",
    "        # SOS- Start of Sentence\n",
    "        # EOS- end of sentence\n",
    "        # UNK- unknown word (unknown\\ below threshold)\n",
    "        self.itos = {0:\"<PAD>\",1:\"<SOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n",
    "        #string to int tokens\n",
    "        #its reverse dict self.itos\n",
    "        self.stoi = {v:k for k,v in self.itos.items()}\n",
    "        self.freq_threshold = freq_threshold\n",
    "        \n",
    "    def __len__(self):\n",
    "      return len(self.itos)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        return [token.text.lower() for token in spacy_eng.tokenizer(text)]\n",
    "    \n",
    "    def build_vocab(self, sentence_list):\n",
    "        frequencies = Counter()\n",
    "        idx = 4\n",
    "        for index,sentence in enumerate(sentence_list):\n",
    "\n",
    "            for word in self.tokenize(sentence):\n",
    "                frequencies[word] += 1\n",
    "                \n",
    "                #add the word to the vocab if it reaches minum frequecy threshold\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    if idx > 0 and idx % 1000==0:\n",
    "                        print(f\"Added {idx} words to vocab\")\n",
    "                    idx += 1\n",
    "            if index>0 and index%1000==0:\n",
    "                print(f\"Iterated {index} sentences\")\n",
    "             \n",
    "\n",
    "        print(f\"Done, added {idx-1} words to vocabulary\")\n",
    "    \n",
    "    def numericalize(self,text):\n",
    "        \"\"\" For each word in the text corresponding index token for that word form the vocab built as list \"\"\"\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        result = [ self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text ]\n",
    "        return result"
   ],
   "outputs": [],
   "metadata": {
    "id": "6YUFskmKSJlD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset custom class"
   ],
   "metadata": {
    "id": "rkgLLalGAHXn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pickle\n",
    "class COCODataset(Dataset):\n",
    "    \"\"\"\n",
    "    COCODataset\n",
    "    \"\"\"\n",
    "    def __init__(self,root_dir,annotation_file,transform=None,freq_threshold=5,\n",
    "                 load_vocab=False, vocab_loc = \"vocab.pkl\"):\n",
    "      \"\"\"\n",
    "      can use load_vocab to use a previously created vocabulary (time saving feature)\n",
    "      freq_threshold: words with a count below this number will be marked as <UNK>\n",
    "      \"\"\"\n",
    "      self.root_dir = root_dir\n",
    "      self.coco = COCO(annotation_file)\n",
    "      self.transform = transform\n",
    "      self.cap_max_size = 0\n",
    "      #Get image and caption colum from the dataframe\n",
    "      self.imgs = []\n",
    "      self.captions = []\n",
    "      for idx,ann in enumerate(self.coco.anns.values()):\n",
    "        self.imgs.append(self.coco.loadImgs((ann['image_id']))[0][\"file_name\"])\n",
    "        self.captions.append(ann['caption'])\n",
    "        if (idx) % 1000 == 0 and idx>0:\n",
    "          print(f\"Processed {idx} images and captions\")\n",
    "      print(\"Finished processing images and captions\")\n",
    "      print(f\"Got:{len(set(self.imgs))} pictures with {len(self.captions)} captions!\")\n",
    "      \n",
    "      #Initialize vocabulary and build vocab\n",
    "      if load_vocab:\n",
    "        with open(vocab_loc, \"rb\") as source:\n",
    "          self.vocab = pickle.load(source)\n",
    "        print(f\"Loaded vocabulary from {vocab_loc}\")\n",
    "      \n",
    "      else:\n",
    "        print(\"Build vocabulary\")\n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocab(self.captions)\n",
    "        print(\"Finished building vocabulary\")\n",
    "        with open(vocab_loc, \"wb\") as dest:\n",
    "          pickle.dump(self.vocab, dest)\n",
    "      \n",
    "      print(f\"Using {len(self.vocab)} words\")\n",
    "    \n",
    "    def __len__(self):\n",
    "      return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "      caption = self.captions[idx]\n",
    "      img_name = self.imgs[idx]\n",
    "      img_location = os.path.join(self.root_dir,img_name)\n",
    "      img = Image.open(img_location).convert(\"RGB\")\n",
    "      \n",
    "      #apply the transfromation to the image\n",
    "      if self.transform:\n",
    "          img = self.transform(img)\n",
    "      \n",
    "      #numericalize the caption text\n",
    "      caption_vec = [self.vocab.stoi[\"<SOS>\"]]\n",
    "      caption_vec.extend(self.vocab.numericalize(caption))\n",
    "      caption_vec.append(self.vocab.stoi[\"<EOS>\"])\n",
    "      \n",
    "      return img, torch.tensor(caption_vec,dtype=torch.long)"
   ],
   "outputs": [],
   "metadata": {
    "id": "Y19oWTAsWmEC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataloader creation"
   ],
   "metadata": {
    "id": "wDqCBk-KCBkw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# define a transformation to add some noise and variance to our images\n",
    "transformation = transforms.Compose([transforms.Resize((299,299), Image.NEAREST),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.RandomVerticalFlip(),\n",
    "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                                      ])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/yandex/DLW2021/pelegv/anaconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py:280: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "metadata": {
    "id": "pNHTCZ8ZcXGQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "def new_collate(batch):\n",
    "    (imgs, targets) = zip(*batch)\n",
    "    imgs = [x.unsqueeze(0) for x in imgs]\n",
    "    imgs = torch.cat(imgs,dim=0)\n",
    "    targets_lens = [len(target) for target in targets]\n",
    "    targets_pad = pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "\n",
    "    return imgs, targets_pad, targets_lens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "dataset =  COCODataset(\n",
    "    root_dir = \"/home/yandex/DLW2021/davidhay/coco/val2017\",\n",
    "    annotation_file= \"/home/yandex/DLW2021/davidhay/coco/annotations/captions_val2017.json\",\n",
    "    transform=transformation,\n",
    "    freq_threshold=5,\n",
    "    load_vocab=False\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "id": "igD_-Qs7oFTz"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "print(dataset.vocab.stoi) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3, 'a': 4, '.': 5, 'in': 6, 'of': 7, 'woman': 8, 'the': 9, 'bench': 10, 'with': 11, 'an': 12, 'and': 13, 'toilet': 14, 'office': 15, 'white': 16, 'on': 17, 'two': 18, 'cat': 19, 'between': 20, 'at': 21, 'man': 22, 'is': 23, 'motorcycle': 24, 'parked': 25, 'standing': 26, 'sitting': 27, 'group': 28, 'to': 29, 'their': 30, 'his': 31, 'front': 32, 'motorcycles': 33, 'next': 34, 'kitchen': 35, 'it': 36, 'dog': 37, 'desk': 38, 'black': 39, ',': 40, 'street': 41, 'computer': 42, 'sky': 43, 'person': 44, 'up': 45, 'bathroom': 46, 'has': 47, 'sink': 48, 'are': 49, 'young': 50, 'yellow': 51, 'small': 52, 'picture': 53, 'giraffe': 54, 'women': 55, 'people': 56, 'oranges': 57, 'bowl': 58, 'vase': 59, 'water': 60, 'close': 61, 'open': 62, 'top': 63, 'zebra': 64, 'looking': 65, 'cupcake': 66, 'plane': 67, 'through': 68, 'field': 69, 'train': 70, 'sits': 71, 'truck': 72, 'door': 73, 'car': 74, 'behind': 75, 'bed': 76, 'flying': 77, 'kites': 78, 'room': 79, 'lot': 80, 'filled': 81, 'station': 82, 'near': 83, 'waiting': 84, 'down': 85, 'phone': 86, 'this': 87, 'shower': 88, 'cabinets': 89, 'men': 90, \"'s\": 91, 'walls': 92, 'mirror': 93, 'wall': 94, 'red': 95, 'old': 96, 'blue': 97, 'striped': 98, 'counter': 99, 'that': 100, 'be': 101, 'airplane': 102, 'restroom': 103, 'monitors': 104, 'cars': 105, 'by': 106, 'while': 107, 'parking': 108, 'scene': 109, 'tub': 110, 'head': 111, 'appliances': 112, 'banana': 113, 'bobble': 114, 'side': 115, 'as': 116, 'light': 117, 'one': 118, 'very': 119, 'road': 120, 'large': 121, 'runway': 122, 'four': 123, 'horse': 124, 'off': 125, 'city': 126, 'plate': 127, 'animals': 128, 'refrigerator': 129, 'eating': 130, 'wearing': 131, 'inside': 132, 'clean': 133, 'seat': 134, 'dirt': 135, 'green': 136, 'some': 137, 'watching': 138, 'tent': 139, 'bathtub': 140, 'cake': 141, 'bike': 142, 'piece': 143, 'fork': 144, ' ': 145, 'bird': 146, 'space': 147, 'colorful': 148, 'dining': 149, 'hood': 150, 'area': 151, 'riding': 152, 'living': 153, 'laptop': 154, 'from': 155, 'jeep': 156, 'desktop': 157, 'another': 158, 'walking': 159, 'sleeping': 160, 'trees': 161, 'them': 162, 'around': 163, 'vanity': 164, 'empty': 165, 'bicycle': 166, 'display': 167, 'lights': 168, 'table': 169, 'pink': 170, 'for': 171, 'floor': 172, 'toy': 173, 'air': 174, 'traffic': 175, 'grass': 176, 'public': 177, 'paper': 178, 'planes': 179, 'home': 180, 'into': 181, 'camera': 182, 'park': 183, 'under': 184, 'middle': 185, 'laying': 186, 'intersection': 187, 'past': 188, 'beach': 189, 'other': 190, 'background': 191, 'chairs': 192, 'tower': 193, 'clock': 194, 'outside': 195, 'gray': 196, 'taking': 197, 'bus': 198, 'multiple': 199, 'tv': 200, 'sinks': 201, 'kitten': 202, 'decorated': 203, 'hat': 204, 'wooden': 205, 'metal': 206, 'stove': 207, 'desert': 208, 'full': 209, 'set': 210, 'photo': 211, 'bananas': 212, 'her': 213, 'out': 214, 'airport': 215, 'walks': 216, 'faucet': 217, 'view': 218, 'driving': 219, 'walk': 220, 'herd': 221, 'brick': 222, 'way': 223, 'back': 224, 'jet': 225, 'takes': 226, 'wood': 227, 'stainless': 228, 'holding': 229, 'food': 230, 'church': 231, 'there': 232, 'stool': 233, 'house': 234, 'three': 235, 'over': 236, 'sit': 237, 'together': 238, 'corner': 239, '-': 240, 'vehicle': 241, 'bunch': 242, 'above': 243, 'bridge': 244, 'being': 245, 'orange': 246, 'posing': 247, 'tarmac': 248, 'several': 249, 'dressed': 250, 'looks': 251, 'stone': 252, 'building': 253, 'teeth': 254, 'vehicles': 255, 'show': 256, 'surrounded': 257, 'stand': 258, 'blowing': 259, 'candles': 260, 'eyes': 261, 'across': 262, 'purple': 263, 'where': 264, 'furniture': 265, 'sidewalk': 266, 'calico': 267, 'bath': 268, 'image': 269, 'many': 270, 'each': 271, 'brown': 272, 'tile': 273, 'fridge': 274, 'beside': 275, 'tall': 276, 'buildings': 277, 'long': 278, 'all': 279, 'flat': 280, 'screen': 281, 'its': 282, 'tiled': 283, 'stall': 284, 'bag': 285, 'gas': 286, 'soap': 287, 'either': 288, 'oven': 289, 'computers': 290, 'screens': 291, '\\n': 292, 'cooking': 293, 'covered': 294, 'passenger': 295, 'bright': 296, 'window': 297, 'lid': 298, 'stands': 299, 'curtain': 300, 'motor': 301, 'cabinet': 302, 'tree': 303, 'bedroom': 304, 'row': 305, 'board': 306, 'helmet': 307, 'silver': 308, 'shown': 309, 'modern': 310, 'steel': 311, 'line': 312, 'day': 313, 'rodeo': 314, 'along': 315, 'track': 316, 'boy': 317, 'skateboard': 318, 'features': 319, 'scooter': 320, 'older': 321, 'couple': 322, 'busy': 323, 'flowers': 324, 'cart': 325, 'been': 326, 'made': 327, 'smoke': 328, 'carrying': 329, 'bikes': 330, 'cigarette': 331, 'he': 332, 'television': 333, 'hats': 334, 'flower': 335, 'glass': 336, 'monitor': 337, 'grey': 338, 'lined': 339, 'girl': 340, 'talking': 341, 'cell': 342, 'grassy': 343, 'ground': 344, 'photograph': 345, 'fruit': 346, 'fighter': 347, 'high': 348, 'clear': 349, 'beautiful': 350, 'painted': 351, 'stopped': 352, 'bar': 353, 'stools': 354, 'tomatoes': 355, 'garage': 356, 'night': 357, 'hand': 358, 'single': 359, 'motorcyclists': 360, 'commercial': 361, 'airplanes': 362, 'something': 363, 'microwave': 364, 'various': 365, 'take': 366, 'overhead': 367, 'bicycles': 368, 'facing': 369, 'preparing': 370, 'box': 371, 'shirt': 372, 'showing': 373, 'lots': 374, 'fence': 375, 'displayed': 376, 'look': 377, 'sheep': 378, 'tie': 379, 'elderly': 380, 'playing': 381, 'container': 382, 'coming': 383, 'gravel': 384, 'pizza': 385, 'counters': 386, 'only': 387, 'crowd': 388, 'restaurant': 389, 'keyboard': 390, 'chair': 391, 'dirty': 392, 'trash': 393, 'hanging': 394, 'leaves': 395, 'ceiling': 396, 'smiling': 397, 'during': 398, 'snowy': 399, 'apples': 400, 'sign': 401, 'pole': 402, 'doors': 403, 'run': 404, 'shop': 405, 'fancy': 406, 'snow': 407, 'flies': 408, 'mirrors': 409, 'cloudy': 410, 'knife': 411, 'placed': 412, 'jacket': 413, 'little': 414, 'child': 415, 'cycles': 416, 'jumbo': 417, 'landing': 418, 'like': 419, 'vintage': 420, 'colored': 421, 'stop': 422, 'face': 423, 'time': 424, 'towels': 425, 'hanger': 426, 'style': 427, 'plants': 428, 'jets': 429, 'leaving': 430, 'different': 431, 'ocean': 432, 'using': 433, 'windows': 434, 'big': 435, 'nice': 436, 'meter': 437, 'seats': 438, 'overlooking': 439, 'against': 440, 'him': 441, 'ticket': 442, 'spoon': 443, 'have': 444, 'items': 445, 'toaster': 446, 'curb': 447, 'going': 448, 'boat': 449, 'slope': 450, 'peeled': 451, 'trail': 452, 'doorway': 453, 'bottle': 454, 'hill': 455, 'coffee': 456, 'setting': 457, 'edge': 458, 'shiny': 459, 'jumping': 460, 'dishwasher': 461, 'making': 462, 'cup': 463, 'toothbrushes': 464, 'working': 465, 'moped': 466, 'lady': 467, 'shows': 468, 'luggage': 469, 'rack': 470, 'someone': 471, 'tiles': 472, 'lake': 473, 'not': 474, 'crossing': 475, 'umbrellas': 476, 'just': 477, 'skate': 478, 'graffiti': 479, 'books': 480, 'doing': 481, 'umbrella': 482, 'shaped': 483, 'tiny': 484, 'country': 485, 'reflection': 486, 'moon': 487, 'hair': 488, 'chandelier': 489, 'fixtures': 490, 'engine': 491, 'colors': 492, 'pattern': 493, 'scooters': 494, 'cover': 495, 'pipes': 496, 'double': 497, 'faces': 498, 'lit': 499, 'roll': 500, 'giraffes': 501, 'distance': 502, 'police': 503, 'military': 504, 'number': 505, 'dark': 506, 'shot': 507, 'traveling': 508, 'control': 509, 'work': 510, 'adult': 511, 'baby': 512, 'rides': 513, 'mounted': 514, 'reaching': 515, 'attached': 516, 'fashioned': 517, 'aircraft': 518, 'pedestrians': 519, 'toilets': 520, 'biplane': 521, 'propeller': 522, 'wires': 523, 'dogs': 524, 'brushing': 525, 'fire': 526, 'hydrant': 527, 'broken': 528, 'officer': 529, 'rest': 530, 'selfie': 531, 'things': 532, 'narrow': 533, 'who': 534, 'plant': 535, 'airliner': 536, 'tail': 537, 'well': 538, 'pot': 539, 'pan': 540, 'jetliner': 541, 'mountain': 542, 'missing': 543, 'bread': 544, 'eggs': 545, 'guy': 546, 'remote': 547, 'leaning': 548, 'market': 549, 'himself': 550, 'rear': 551, 'girls': 552, 'smoking': 553, 'no': 554, 'tank': 555, 'statue': 556, 'cleaning': 557, 'couches': 558, 'island': 559, 'topped': 560, 'riders': 561, 'chained': 562, 'holder': 563, 'cross': 564, 'race': 565, 'about': 566, 'or': 567, 'bikers': 568, 'cluttered': 569, 'gathering': 570, 'half': 571, 'scarf': 572, 'used': 573, 'running': 574, 'drinking': 575, 'ready': 576, 'dish': 577, 'shelves': 578, 'pieces': 579, 'collection': 580, 'shelf': 581, 'cut': 582, 'lane': 583, 'highway': 584, 'sea': 585, 'mountains': 586, 'gate': 587, 'terminal': 588, 'town': 589, 'range': 590, 'lying': 591, 'plastic': 592, 'lines': 593, 'hotel': 594, 'can': 595, 'pots': 596, 'feet': 597, 'seen': 598, 'strip': 599, 'antique': 600, 'store': 601, 'puppy': 602, 'guitar': 603, 'stuffed': 604, 'children': 605, 'ascending': 606, 'cutting': 607, 'underneath': 608, 'wing': 609, 'crowded': 610, 'basket': 611, 'painting': 612, 'pair': 613, 'gold': 614, 'floors': 615, 'mopeds': 616, 'bucket': 617, 'fan': 618, 'enclosure': 619, 'containing': 620, 'towel': 621, 'bottom': 622, 'zoo': 623, 'see': 624, 'urinal': 625, 'fly': 626, 'compact': 627, 'storage': 628, 'freezer': 629, 'basin': 630, 'bowls': 631, 'suit': 632, 'rider': 633, 'break': 634, 'holds': 635, 'speed': 636, 'horses': 637, 'left': 638, 'getting': 639, 'taken': 640, 'pans': 641, 'flag': 642, 'tables': 643, 'lap': 644, 'others': 645, 'electronic': 646, 'few': 647, 'center': 648, 'cow': 649, 'slices': 650, 'outdoor': 651, '\"': 652, 'commode': 653, 'event': 654, 'urinals': 655, 'pavement': 656, 'post': 657, 'new': 658, 'sports': 659, 'low': 660, 'american': 661, 'opened': 662, 'game': 663, 'mouse': 664, 'tricks': 665, 'river': 666, 'poster': 667, 'signs': 668, 'doll': 669, 'these': 670, 'drawn': 671, 'business': 672, 'bicyclist': 673, 'roof': 674, 'ride': 675, 'yard': 676, 'herding': 677, 'trucks': 678, 'utensils': 679, 'flock': 680, 'pulled': 681, 'plain': 682, 'elephants': 683, 'gathered': 684, 'they': 685, 'passing': 686, 'apartment': 687, 'ski': 688, 'rail': 689, 'rose': 690, 'candle': 691, 'bidet': 692, 'tracks': 693, 'benches': 694, 'buses': 695, 'paved': 696, 'cats': 697, 'color': 698, 'transit': 699, 'railing': 700, 'book': 701, 'walkway': 702, 'residential': 703, 'pretty': 704, 'dishes': 705, 'closet': 706, 'towards': 707, 'marble': 708, 'dispenser': 709, 'van': 710, 'featuring': 711, 'end': 712, 'interior': 713, 'pen': 714, 'gear': 715, 'foreground': 716, 'helmets': 717, 'huge': 718, 'wheels': 719, 'rug': 720, 'equipment': 721, 'museum': 722, 'windshield': 723, 'animal': 724, 'subway': 725, 'lunch': 726, 'platform': 727, 'square': 728, 'both': 729, 'hardwood': 730, 'which': 731, 'smart': 732, 'resting': 733, 'soup': 734, 'rice': 735, 'ball': 736, 'wide': 737, 'good': 738, 'formation': 739, 'leash': 740, 'trick': 741, 'same': 742, 'surface': 743, 'having': 744, 'variety': 745, 'prop': 746, 'tabby': 747, 'closed': 748, 'rests': 749, 'wheel': 750, 'library': 751, 'tan': 752, 'tire': 753, 'cage': 754, 'suitcases': 755, 'stacked': 756, 'brush': 757, 'racing': 758, 'toys': 759, 'herself': 760, 'contains': 761, 'what': 762, 'locked': 763, 'bottles': 764, 'ten': 765, 'curtains': 766, 'closeup': 767, 'forest': 768, 'staring': 769, 'crosswalk': 770, 'decorations': 771, 'urban': 772, 'passengers': 773, 'asian': 774, 'eat': 775, 'golden': 776, 'scissors': 777, 'kids': 778, 'woods': 779, 'boats': 780, 'alongside': 781, 'rain': 782, 'decker': 783, 'drink': 784, 'signal': 785, 'blurry': 786, 'touching': 787, 'grazing': 788, 'tongue': 789, 'alone': 790, 'perched': 791, 'tied': 792, 'floating': 793, 'mural': 794, 'railroad': 795, 'pulling': 796, 'visible': 797, 'rural': 798, 'onto': 799, 'birthday': 800, 'was': 801, 'party': 802, 'dress': 803, 'harbor': 804, 'poles': 805, 'upon': 806, 'sunny': 807, 'chocolate': 808, 'bars': 809, 'chicken': 810, 'birds': 811, 'advertising': 812, 'branch': 813, 'power': 814, 'goat': 815, 'arranged': 816, 'hills': 817, 'drives': 818, 'school': 819, 'stopping': 820, 'streets': 821, 'waits': 822, 'parade': 823, 'mouth': 824, 'right': 825, 'reflected': 826, 'eye': 827, 'get': 828, 'trailer': 829, 'shade': 830, 'ducks': 831, 'far': 832, 'reads': 833, 'reading': 834, 'pictures': 835, 'put': 836, 'stairs': 837, 'pointing': 838, 'crosses': 839, 'watch': 840, 'away': 841, 'st.': 842, 'eats': 843, 'pictured': 844, 'tour': 845, 'commuter': 846, 'construction': 847, 'ornate': 848, 'sand': 849, 'beneath': 850, 'go': 851, 'tunnel': 852, 'design': 853, 'telephone': 854, 'directions': 855, 'lush': 856, 'feeding': 857, 'heads': 858, 'goats': 859, 'land': 860, 'jackets': 861, 'st': 862, 'sticking': 863, 'leans': 864, 'neck': 865, 'zebras': 866, 'wait': 867, 'baseball': 868, 'meat': 869, 'crouched': 870, 'lamp': 871, 'travels': 872, 'pass': 873, 'makes': 874, 'here': 875, 'appears': 876, 'east': 877, 'roadway': 878, 'says': 879, 'drive': 880, 'rusty': 881, 'approaching': 882, 'pants': 883, 'structure': 884, 'moving': 885, 'trying': 886, 'direction': 887, 'swans': 888, 'swimming': 889, 'apple': 890, 'path': 891, 'concrete': 892, 'fish': 893, 'curve': 894, 'lawn': 895, 'sandy': 896, 'shopping': 897, 'uniform': 898, 'body': 899, 'quiet': 900, 'geese': 901, 'nearby': 902, 'stoplight': 903, 'docked': 904, 'wooded': 905, 'newspaper': 906, 'dry': 907, 'family': 908, 'exit': 909, 'dead': 910, 'we': 911, 'part': 912, 'opposite': 913, 'electric': 914, 'sad': 915, 'petting': 916, 'bushes': 917, 'fenced': 918, 'wet': 919, 'hay': 920, 'rock': 921, 'rocks': 922, 'downtown': 923, 'toward': 924, 'milk': 925, 'itself': 926, 'i': 927, 'cups': 928, 'seated': 929, 'neon': 930, 'logo': 931, 'palm': 932, 'trains': 933, 'below': 934, 'flags': 935, 'located': 936, 'legs': 937, 'climbing': 938, 'boots': 939, 'coat': 940, 'stores': 941, 'dusk': 942, 'pigeons': 943, 'entrance': 944, 'do': 945, 'atop': 946, 'chinese': 947, 'hands': 948, 'cute': 949, 'short': 950, 'sides': 951, 'watches': 952, 'pet': 953, 'multi': 954, 'shining': 955, 'five': 956, 'pasture': 957, 'skirt': 958, 'frame': 959, 'pedestrian': 960, 'costume': 961, 'passes': 962, 'male': 963, 'chain': 964, 'sticks': 965, 'zone': 966, 'among': 967, 'advertisement': 968, 'rocky': 969, 'freeway': 970, 'lone': 971, 'wild': 972, 'sunset': 973, 'object': 974, 'pile': 975, 'straw': 976, 'neighborhood': 977, 'bush': 978, 'pulls': 979, 'ave': 980, '/': 981, 'rainy': 982, 'beak': 983, 'almost': 984, 'elevated': 985, 'block': 986, 'locomotive': 987, 'posted': 988, 'speeding': 989, 'upside': 990, 'overpass': 991, 'map': 992, 'travelling': 993, 'writing': 994, 'seagulls': 995, 'shore': 996, 'round': 997, 'point': 998, 'cows': 999, 'wire': 1000, 'service': 1001, 'indoor': 1002, 'cap': 1003, 'dock': 1004, 'metro': 1005, 'pickup': 1006, 'art': 1007, 'cement': 1008, 'barrier': 1009, 'rusted': 1010, 'boarding': 1011, 'funny': 1012, 'type': 1013, 'brightly': 1014, 'hydrants': 1015, 'rainbow': 1016, 'suspended': 1017, 'feeder': 1018, 'stuck': 1019, 'base': 1020, 'taxi': 1021, 'rails': 1022, 'shrubbery': 1023, 'landscape': 1024, 'sculpture': 1025, 'connected': 1026, 'opening': 1027, 'cargo': 1028, 'santa': 1029, 'including': 1030, 'installed': 1031, 'houses': 1032, 'transportation': 1033, 'landed': 1034, 'picnic': 1035, 'loading': 1036, 'ramp': 1037, 'place': 1038, 'sun': 1039, 'barn': 1040, 'mother': 1041, 'pushing': 1042, 'reflecting': 1043, 'pool': 1044, 'smaller': 1045, 'pond': 1046, 'steam': 1047, 'marsh': 1048, 'paint': 1049, 'wool': 1050, 'swan': 1051, 'leaf': 1052, 'vandalized': 1053, 'farm': 1054, 'clouds': 1055, 'multicolored': 1056, 'laptops': 1057, 'fountain': 1058, 'bat': 1059, 'garden': 1060, 'bare': 1061, 'extremely': 1062, 'freight': 1063, 'haired': 1064, 'iron': 1065, 'trunk': 1066, 'skyline': 1067, 'letting': 1068, 'roads': 1069, 'dessert': 1070, 'railway': 1071, 'giant': 1072, 'handicap': 1073, 'six': 1074, 'names': 1075, 'spot': 1076, 'wilderness': 1077, 'driveway': 1078, 'district': 1079, 'cast': 1080, 'link': 1081, 'bay': 1082, 'posed': 1083, 'persons': 1084, 'ripe': 1085, 'sized': 1086, 'know': 1087, 'meters': 1088, 'containers': 1089, 'christmas': 1090, 'guys': 1091, 'decked': 1092, 'cones': 1093, 'kneeling': 1094, 'wings': 1095, 'terrain': 1096, 'duck': 1097, 'adults': 1098, 'slanted': 1099, 'enclosed': 1100, 'focus': 1101, \"n't\": 1102, 'sticker': 1103, 'written': 1104, 'displaying': 1105, 'branches': 1106, 'feed': 1107, 'trolley': 1108, 'steps': 1109, 'collage': 1110, 'photos': 1111, 'feathers': 1112, 'language': 1113, 'potted': 1114, \"'\": 1115, 'billboard': 1116, 'playground': 1117, 'points': 1118, 'drivers': 1119, 'rolls': 1120, 'bending': 1121, 'banner': 1122, 'avenue': 1123, 'teddy': 1124, 'partially': 1125, 'bear': 1126, 'habitat': 1127, 'lighted': 1128, 'elephant': 1129, 'surfboard': 1130, 'give': 1131, 'cartoon': 1132, 'arrow': 1133, 'turn': 1134, 'illuminated': 1135, 'seagull': 1136, 'tray': 1137, 'stream': 1138, 'use': 1139, 'cowboy': 1140, 'exhibit': 1141, 'stops': 1142, 'pick': 1143, 'come': 1144, 'relaxing': 1145, 'curled': 1146, 'shops': 1147, 'lounge': 1148, 'couch': 1149, 'hillside': 1150, 'boys': 1151, 'countryside': 1152, 'fallen': 1153, 'suitcase': 1154, 'weeds': 1155, 'enjoying': 1156, 'shallow': 1157, 'arms': 1158, 'fluffy': 1159, 'beard': 1160, 'stares': 1161, 'calf': 1162, 'drinks': 1163, 'ledge': 1164, 'players': 1165, 'court': 1166, 'cattle': 1167, 'poses': 1168, 'sill': 1169, 'mini': 1170, 'stretches': 1171, 'indoors': 1172, 'paws': 1173, 'army': 1174, 'toothbrush': 1175, '..': 1176, 'bent': 1177, 'shoulder': 1178, 'graze': 1179, 'frisbee': 1180, 'prepares': 1181, 'patio': 1182, 'enter': 1183, 'gather': 1184, 'world': 1185, 'still': 1186, 'barbed': 1187, 'glasses': 1188, 'attempting': 1189, 'stick': 1190, 'plays': 1191, 'gentleman': 1192, 'blanket': 1193, 'carts': 1194, 'beer': 1195, 'lay': 1196, 'nap': 1197, 'bags': 1198, 'foot': 1199, 'hang': 1200, 'pack': 1201, 'shoe': 1202, 'sniffing': 1203, 'bears': 1204, 'thick': 1205, 'log': 1206, 'hall': 1207, 'buddha': 1208, 'shady': 1209, 'happy': 1210, 'hot': 1211, 'nose': 1212, 'tow': 1213, 'piled': 1214, 'perches': 1215, 'travel': 1216, 'coach': 1217, 'basketball': 1218, 'raised': 1219, 'sail': 1220, 'deer': 1221, 'loaded': 1222, 'aged': 1223, 'meal': 1224, 'without': 1225, 'load': 1226, 'antelope': 1227, 'dump': 1228, 'giving': 1229, 'lighting': 1230, 'hauling': 1231, 'foggy': 1232, 'stating': 1233, 'clothing': 1234, 'tourists': 1235, 'panda': 1236, 'wheeled': 1237, 'husky': 1238, 'semi': 1239, 'numerous': 1240, 'sale': 1241, 'shoes': 1242, 'deck': 1243, 'reach': 1244, 'environment': 1245, 'seating': 1246, 'outdoors': 1247, 'types': 1248, 'backpack': 1249, 'cleaned': 1250, 'so': 1251, 'if': 1252, 'you': 1253, 'pay': 1254, 'straight': 1255, 'your': 1256, 'clothes': 1257, 'polar': 1258, 'abandoned': 1259, 'lays': 1260, 'balloon': 1261, 'waterway': 1262, 'case': 1263, 'bow': 1264, 'rows': 1265, 'groups': 1266, 'backs': 1267, 'peeking': 1268, 'human': 1269, 'catching': 1270, 'canoe': 1271, 'valley': 1272, 'turned': 1273, 'ring': 1274, 'patch': 1275, 'gives': 1276, 'bull': 1277, 'meadow': 1278, 'tags': 1279, 'covers': 1280, 'beige': 1281, 'caught': 1282, 'ship': 1283, 'pier': 1284, 'handing': 1285, 'after': 1286, 'tractor': 1287, 'picking': 1288, 'trunks': 1289, 'forested': 1290, 'ships': 1291, 'but': 1292, 'classic': 1293, 'larger': 1294, 'surf': 1295, 'handle': 1296, 'licking': 1297, 'stack': 1298, 'cases': 1299, 'carry': 1300, 'lies': 1301, 'cellphone': 1302, 'life': 1303, 'photographer': 1304, 'backdrop': 1305, 'ears': 1306, 'player': 1307, 'heels': 1308, 'tusks': 1309, 'make': 1310, 'unique': 1311, 'cub': 1312, 'sunglasses': 1313, 'throwing': 1314, 'surrounding': 1315, 'sets': 1316, 'skateboards': 1317, 'canal': 1318, 'carpet': 1319, 'model': 1320, 'positioned': 1321, 'crate': 1322, 'leading': 1323, 'stickers': 1324, 'following': 1325, 'cruise': 1326, 'kid': 1327, 'ice': 1328, 'pig': 1329, 'female': 1330, 'blankets': 1331, 'am': 1332, 'papers': 1333, 'nicely': 1334, 'suits': 1335, 'observing': 1336, 'doughnut': 1337, 'purse': 1338, 'boxes': 1339, 'hit': 1340, 'canopy': 1341, 'waters': 1342, 'aerial': 1343, 'fun': 1344, 'skis': 1345, 'pose': 1346, 'president': 1347, 'vest': 1348, 'images': 1349, 'cop': 1350, 'briefcase': 1351, 'those': 1352, 'net': 1353, 'runs': 1354, 'fed': 1355, 'horns': 1356, 'really': 1357, 'calm': 1358, 'kitty': 1359, 'statues': 1360, 'donkey': 1361, 'utility': 1362, 'reaches': 1363, 'cream': 1364, 'helping': 1365, 'bank': 1366, 'ferry': 1367, 'port': 1368, 'speaking': 1369, 'shadow': 1370, 'asleep': 1371, 'rubbing': 1372, 'heavy': 1373, 'sofa': 1374, 'portrait': 1375, 'hole': 1376, 'dilapidated': 1377, 'scratching': 1378, 'forward': 1379, 'bride': 1380, 'piercings': 1381, '2': 1382, 'drawing': 1383, 'directly': 1384, 'crane': 1385, 'catch': 1386, 'disc': 1387, 'cab': 1388, 'play': 1389, 'ties': 1390, 'lie': 1391, 'formal': 1392, 'wear': 1393, 'interesting': 1394, 'selling': 1395, 'fishing': 1396, 'marina': 1397, 'covering': 1398, 'sleeps': 1399, 'sailboats': 1400, 'tusked': 1401, 'muddy': 1402, 'wears': 1403, 'ear': 1404, 'vegetation': 1405, 'name': 1406, 'roadside': 1407, 'owner': 1408, 'paw': 1409, 'extended': 1410, 'smiles': 1411, 'cave': 1412, 'garbage': 1413, 'workers': 1414, 'smile': 1415, 'siting': 1416, 'crossed': 1417, 'pin': 1418, 'transport': 1419, 'lobby': 1420, 'uniforms': 1421, 'sneakers': 1422, 'arm': 1423, 'overcast': 1424, 'kite': 1425, 'pillow': 1426, 'goes': 1427, 'cobblestone': 1428, 'spraying': 1429, 'hose': 1430, 'machine': 1431, 'groom': 1432, 'married': 1433, 'class': 1434, 'decorative': 1435, 'stuff': 1436, 'harness': 1437, 'leafy': 1438, 'hiding': 1439, 'spotted': 1440, 'sailing': 1441, 'process': 1442, 'milking': 1443, 'baggage': 1444, 'waves': 1445, 'flooded': 1446, 'foliage': 1447, 'hallway': 1448, 'delivery': 1449, 'rolling': 1450, 'washed': 1451, 'fake': 1452, 'viewing': 1453, 'fur': 1454, 'also': 1455, 'alley': 1456, 'lining': 1457, 'stripe': 1458, 'paddle': 1459, 'followed': 1460, 'attire': 1461, 'checking': 1462, 'microphone': 1463, 'stage': 1464, 'trailers': 1465, 'plaza': 1466, 'jeans': 1467, 'collar': 1468, 'comforter': 1469, 'dairy': 1470, 'sniffs': 1471, 'shaking': 1472, 'dinner': 1473, 'performing': 1474, 'beans': 1475, 'vegetables': 1476, 'skier': 1477, 'shirtless': 1478, 'salad': 1479, 'odd': 1480, 'skateboarding': 1481, 'pitcher': 1482, 'rope': 1483, 'skiing': 1484, 'skateboarder': 1485, 'leads': 1486, 'paddock': 1487, 'eaten': 1488, 'held': 1489, 'puddle': 1490, 'scenic': 1491, 'spread': 1492, 'snowboard': 1493, 'countertop': 1494, 'broccoli': 1495, 'plates': 1496, 'skiers': 1497, 'serving': 1498, 'cheese': 1499, 'intently': 1500, 'team': 1501, 'plaid': 1502, 'foods': 1503, 'jump': 1504, 'sandwich': 1505, 'does': 1506, 'baskets': 1507, 'cook': 1508, 'gloves': 1509, 'throw': 1510, 'carrots': 1511, 'plains': 1512, 'vendor': 1513, 'produce': 1514, 'section': 1515, 'vegetable': 1516, 'skies': 1517, 'boards': 1518, 'served': 1519, 'fruits': 1520, 'freshly': 1521, 'marked': 1522, 'sauce': 1523, 'grab': 1524, 'mid': 1525, 'soccer': 1526, 'mound': 1527, 'follows': 1528, 'cooked': 1529, 'greens': 1530, 'third': 1531, 'assorted': 1532, 'dusty': 1533, 'fresh': 1534, 'displays': 1535, 'ahead': 1536, 'cloth': 1537, 'friends': 1538, 'circle': 1539, 'lens': 1540, 'lift': 1541, 'rim': 1542, 'print': 1543, 'spam': 1544, 'sandwiches': 1545, 'spots': 1546, 'carriage': 1547, 'backyard': 1548, 'wedding': 1549, 'mud': 1550, 'skateboarders': 1551, 'medium': 1552, 'sliding': 1553, 'falling': 1554, 'seven': 1555, 'shoreline': 1556, 'hilly': 1557, 'bikinis': 1558, 'horseback': 1559, 'pillows': 1560, 'lemon': 1561, 'plow': 1562, 'sliced': 1563, 'wicker': 1564, 'laid': 1565, 'hold': 1566, 'downhill': 1567, 'carries': 1568, 'slice': 1569, 'prepared': 1570, 'bite': 1571, 'obstacle': 1572, 'assortment': 1573, 'let': 1574, 'delicious': 1575, 'comes': 1576, 'talk': 1577, 'pipe': 1578, 'shape': 1579, 'course': 1580, 'farmer': 1581, 'pony': 1582, 'mat': 1583, 'chopped': 1584, 'floral': 1585, 'balls': 1586, 'performs': 1587, 'poodle': 1588, 'buggy': 1589, 'washing': 1590, 'donut': 1591, 'leg': 1592, 'watering': 1593, 'catcher': 1594, 'massive': 1595, 'stunt': 1596, 'kind': 1597, 'mostly': 1598, 'slightly': 1599, 'potatoes': 1600, 'goods': 1601, 'squatting': 1602, 'includes': 1603, 'objects': 1604, 'grinding': 1605, 'more': 1606, 'professional': 1607, 'nuts': 1608, 'shrubs': 1609, 'safety': 1610, 'strawberries': 1611, 'fighting': 1612, 'circular': 1613, 'tries': 1614, 'icing': 1615, 'leaping': 1616, 'whole': 1617, 'berries': 1618, 'themselves': 1619, 'kept': 1620, 'spectators': 1621, 'veggies': 1622, 'own': 1623, 'corral': 1624, 'crackers': 1625, 'jars': 1626, 'cafeteria': 1627, 'noodles': 1628, 'growing': 1629, 'deep': 1630, 'skates': 1631, 'skating': 1632, 'approaches': 1633, 'snowboarding': 1634, 'boarder': 1635, 'batter': 1636, 'putting': 1637, 'pastry': 1638, 'snack': 1639, 'african': 1640, 'resort': 1641, 'how': 1642, 'driver': 1643, 'historic': 1644, 'pull': 1645, 'remotes': 1646, 'pitch': 1647, 'series': 1648, 'may': 1649, 'hangs': 1650, 'limb': 1651, 'slopes': 1652, 'platter': 1653, 'beverage': 1654, 'jockeys': 1655, 'frisbees': 1656, 'swinging': 1657, 'flip': 1658, 'snowboarder': 1659, 'cakes': 1660, 'sort': 1661, 'jumps': 1662, 'pasta': 1663, 'disk': 1664, 'cloud': 1665, 'cookies': 1666, 'audience': 1667, 'butter': 1668, 'throws': 1669, 'bunches': 1670, 'coin': 1671, 'jean': 1672, 'glove': 1673, 'matching': 1674, 'she': 1675, 'teenager': 1676, 'mitt': 1677, 'healthy': 1678, 'onions': 1679, 'sugar': 1680, 'grasses': 1681, 'waffle': 1682, 'bagel': 1683, 'toddler': 1684, 'seems': 1685, 'carrot': 1686, 'chew': 1687, 'lemons': 1688, 'llama': 1689, 'peas': 1690, 'cluster': 1691, 'places': 1692, 'such': 1693, 'pineapple': 1694, 'pineapples': 1695, 'relax': 1696, 'patterned': 1697, 'gets': 1698, 'mixed': 1699, 'skiis': 1700, 'action': 1701, 'before': 1702, 'thrown': 1703, 'pears': 1704, 'runner': 1705, 'chasing': 1706, 'wagon': 1707, 'fries': 1708, 'hitting': 1709, 'pitching': 1710, 'swings': 1711, 'mug': 1712, 'tea': 1713, 'leaned': 1714, 'tropical': 1715, 'steep': 1716, 'tasty': 1717, 'ham': 1718, 'powdered': 1719, 'laughing': 1720, 'thing': 1721, 'evergreen': 1722, 'golf': 1723, 'flight': 1724, 'cuts': 1725, 'natural': 1726, 'planters': 1727, 'moves': 1728, 'grazes': 1729, 'mountainous': 1730, 'umpire': 1731, 'swing': 1732, 'individual': 1733, 'pear': 1734, 'peppers': 1735, 'wine': 1736, 'outfit': 1737, 'hamburger': 1738, 'grouped': 1739, 'size': 1740, 'ways': 1741, 'sheet': 1742, 'heading': 1743, 'guiding': 1744, 'frosting': 1745, 'warehouse': 1746, 'kinds': 1747, 'breakfast': 1748, 'catches': 1749, 'grown': 1750, 'pie': 1751, 'stir': 1752, 'juice': 1753, 'cone': 1754, 'fried': 1755, 'skillet': 1756, 'piles': 1757, 'dugout': 1758, 'tag': 1759, 'self': 1760, 'practicing': 1761, 'roller': 1762, 'celery': 1763, 'competition': 1764, 'position': 1765, 'circus': 1766, 'grizzly': 1767, 'catchers': 1768, 'grapes': 1769, 'neatly': 1770, 'peel': 1771, 'bamboo': 1772, 'jockey': 1773, 'sideways': 1774, 'backpacks': 1775, 'league': 1776, 'shirts': 1777, 'frisbe': 1778, 'pastries': 1779, 'furry': 1780, 'ladies': 1781, 'buffalo': 1782, 'amount': 1783, 'competing': 1784, 'balancing': 1785, 'spray': 1786, 'spoons': 1787, 'stunts': 1788, ';': 1789, 'burger': 1790, 'split': 1791, 'pets': 1792, 'blender': 1793, 'friend': 1794, 'races': 1795, 'n': 1796, 'saddle': 1797, 'attempts': 1798, 'fuzzy': 1799, 'trio': 1800, 'tennis': 1801, 'motion': 1802, 'donuts': 1803, 'video': 1804, 'wii': 1805, 'knees': 1806, 'personal': 1807, 'bacon': 1808, 'tomato': 1809, 'sausage': 1810, 'blonde': 1811, 'garnish': 1812, 'lettuce': 1813, 'frosted': 1814, 'racket': 1815, 'crust': 1816, 'muffin': 1817, 'wave': 1818, 'sprinkles': 1819, 'coast': 1820, 'surfers': 1821, 'surfer': 1822, 'kicking': 1823, 'photographs': 1824, 'devices': 1825, 'surfing': 1826, 'adorable': 1827, 'match': 1828, 'games': 1829, 'onion': 1830, 'condiments': 1831, 'than': 1832, 'puts': 1833, 'grilled': 1834, 'french': 1835, 'dresser': 1836, 'pepperoni': 1837, 'sharing': 1838, 'propped': 1839, 'racquet': 1840, 'sheets': 1841, 'fast': 1842, 'kick': 1843, 'diner': 1844, 'twin': 1845, '6': 1846, 'artistic': 1847, 'rowing': 1848, 'folded': 1849, 'hotdog': 1850, 'beef': 1851, 'leather': 1852, 'flooring': 1853, 'hard': 1854, 'when': 1855, 'suite': 1856, 'serve': 1857, 'mustard': 1858, 'pizzas': 1859, 'shorts': 1860, 'thin': 1861, 'serves': 1862, 'porch': 1863, 'desserts': 1864, 'beds': 1865, 'figures': 1866, 'winter': 1867, 'stars': 1868, 'bearded': 1869, 'doughnuts': 1870, 'glazed': 1871, 'cushions': 1872, 'macaroni': 1873, 'cold': 1874, 'guard': 1875, 'bun': 1876, 'wetsuit': 1877, 'warning': 1878, 'system': 1879, 'toppings': 1880, 'surfboards': 1881, 'try': 1882, 'decor': 1883, 'bookcase': 1884, 'baked': 1885, 'flowered': 1886, 'eyed': 1887, 'pregnant': 1888, 'speakers': 1889, 'teams': 1890, 'hotdogs': 1891, 'corn': 1892, 'nintendo': 1893, 'controller': 1894, 'holes': 1895, 'hooked': 1896, 'soda': 1897, 'fingers': 1898, 'brushes': 1899, 'cardboard': 1900, 'pad': 1901, 'hospital': 1902, 'para': 1903, 'cramped': 1904, 'trays': 1905, 'wiping': 1906, 'saying': 1907, 'portion': 1908, 'chips': 1909, 'bikini': 1910, 'mobile': 1911, 'ketchup': 1912, '?': 1913, 'much': 1914, 'messy': 1915, 'curved': 1916, 'snowboards': 1917, 'wrapped': 1918, 'fireplace': 1919, 'napkin': 1920, 'gym': 1921, 'newly': 1922, 'dozens': 1923, 'machines': 1924, 'cafe': 1925, 'pickles': 1926, 'spinach': 1927, 'foil': 1928, 'strawberry': 1929, 'oval': 1930, 'bedspread': 1931, 'framed': 1932, 'strange': 1933, 'balance': 1934, 'balances': 1935, 'classroom': 1936, 'grocery': 1937, 'story': 1938, 'stormy': 1939, 'chalkboard': 1940, 't': 1941, 'hits': 1942, 'skills': 1943, 'surfs': 1944, 'cool': 1945, 'supplies': 1946, '3': 1947, 'cupcakes': 1948, 'panoramic': 1949, 'olives': 1950, 'factory': 1951, 'dried': 1952, 'wake': 1953, 'topping': 1954, 'setup': 1955, 'enjoy': 1956, 'glides': 1957, 'unable': 1958, 'panel': 1959, 'lifting': 1960, 'biting': 1961, 'unmade': 1962, 'robe': 1963, 'buns': 1964, 'passed': 1965, 'homemade': 1966, 'belt': 1967, 'rackets': 1968, 'bakery': 1969, 'snowboarders': 1970, 'toast': 1971, 'pepper': 1972, 'besides': 1973, 'airborne': 1974, 'ribbon': 1975, 'football': 1976, 'headphones': 1977, 'electronics': 1978, 'coats': 1979, 'sailboat': 1980, 'bedding': 1981, 'netting': 1982, 'rough': 1983, 'mosquito': 1984, 'breaking': 1985, 'flipping': 1986, 'boogie': 1987, 'draped': 1988, 'return': 1989, 'volley': 1990, 'grill': 1991, 'interacting': 1992, 'fashion': 1993, 'tin': 1994, 'baking': 1995, 'burrito': 1996, 'extra': 1997, 'surround': 1998, 'mushrooms': 1999, 'phones': 2000, 'raw': 2001, 'layer': 2002, 'silverware': 2003, 'beverages': 2004, 'crest': 2005, 'paddling': 2006, 'holders': 2007, 'controllers': 2008, 'remodeled': 2009, 'toothpaste': 2010, 'pepsi': 2011, 'egg': 2012, 'tops': 2013, 'toasted': 2014, 'incoming': 2015, 'offering': 2016, 'tube': 2017, 'someones': 2018, 'dip': 2019, 'had': 2020, 'skater': 2021, 'tooth': 2022, 'chili': 2023, 'clay': 2024, 'youth': 2025, 'socks': 2026, 'sweater': 2027, 'shines': 2028, 'apron': 2029, 'chefs': 2030, 'upward': 2031, 'stripped': 2032, 'returning': 2033, 'canadian': 2034, 'prepare': 2035, 'buffet': 2036, 'rings': 2037, 'device': 2038, 'early': 2039, 'teaching': 2040, 'words': 2041, 'cookie': 2042, 'sausages': 2043, 'site': 2044, 'customers': 2045, 'chewing': 2046, 'grabbing': 2047, 'slowly': 2048, 'within': 2049, 'mattress': 2050, 'sleep': 2051, 'crashing': 2052, 'dozen': 2053, 'controls': 2054, 'wetsuits': 2055, 'liquid': 2056, 'bald': 2057, 'boarders': 2058, 'simple': 2059, 'happily': 2060, 'ingredients': 2061, 'mugs': 2062, 'younger': 2063, 'metallic': 2064, 'alcohol': 2065, 'roasting': 2066, 'accents': 2067, 'booth': 2068, 'reception': 2069, 'fixing': 2070, 'tools': 2071, 'doubles': 2072, 'stadium': 2073, 'clown': 2074, 'tram': 2075, 'vases': 2076, 'wind': 2077, 'washer': 2078, 'bats': 2079, 'cereal': 2080, 'outfield': 2081, 'combination': 2082, 'foreign': 2083, 'airline': 2084, '4': 2085, 'printed': 2086, 'electrical': 2087, 'stocked': 2088, 'honey': 2089, 'extends': 2090, 'placing': 2091, 'midst': 2092, 'sails': 2093, 'slicing': 2094, 'headed': 2095, 'cheesy': 2096, 'starting': 2097, 'entertainment': 2098, 'listening': 2099, 'steeple': 2100, 'dryer': 2101, 'makeup': 2102, 'operating': 2103, 'calculator': 2104, 'flown': 2105, 'chef': 2106, 'tossing': 2107, 'stripes': 2108, 'second': 2109, 'clocks': 2110, 'oddly': 2111, 'iphone': 2112, 'turkey': 2113, 'faced': 2114, 'roman': 2115, 'bunny': 2116, 'arrows': 2117, 'message': 2118, 'jersey': 2119, 'lamps': 2120, 'tilted': 2121, 'knee': 2122, 'blurred': 2123, 'text': 2124, 'item': 2125, 'parents': 2126, 'skyscraper': 2127, 'batting': 2128, 'snacks': 2129, 'stairway': 2130, 'security': 2131, 'products': 2132, 'towering': 2133, 'tournament': 2134, 'soldiers': 2135, 'outfits': 2136, 'ben': 2137, 'london': 2138, 'talks': 2139, 'goggles': 2140, 'weather': 2141, 'tongs': 2142, 'dimly': 2143, 'arrangement': 2144, 'bookshelf': 2145, 'hung': 2146, '  ': 2147, 'stance': 2148, 'balloons': 2149, 'referee': 2150, 'cooks': 2151, 'packed': 2152, 'contents': 2153, 'movie': 2154, 'typing': 2155, 'village': 2156, 'built': 2157, 'elaborate': 2158, 'finger': 2159, 'button': 2160, 'sushi': 2161, 'blow': 2162, 'drying': 2163, 'courtyard': 2164, 'frozen': 2165, 'designed': 2166, 'string': 2167, 'heart': 2168, 'wedges': 2169, 'diced': 2170, 'cellphones': 2171, 'kettle': 2172, 'tidy': 2173, 'jar': 2174, 'junk': 2175, 'played': 2176, 'refrigerators': 2177, 'include': 2178, 'numbers': 2179, 'dumpster': 2180, 'works': 2181, 'uses': 2182, 'windowsill': 2183, 'digital': 2184, 'buttons': 2185, 'music': 2186, 'numerals': 2187, 'barefoot': 2188, 'accessories': 2189, 'complete': 2190, 'females': 2191, 'pens': 2192, 'granite': 2193, 'pedestal': 2194, 'handles': 2195, 'oil': 2196, 't.v': 2197, 'hairbrush': 2198, 'viewed': 2199, 'cupboards': 2200, 'were': 2201, 'bleachers': 2202, 'practice': 2203, 'males': 2204, 'rises': 2205, 'printer': 2206, 'athletes': 2207, 'jewelry': 2208, 'legged': 2209, 'swung': 2210, 'celebrating': 2211, 'elegant': 2212, 'princess': 2213, 'care': 2214, 'chopsticks': 2215, 'else': 2216, 'den': 2217, 'figurines': 2218, 'parachute': 2219, 'dolls': 2220, 'windy': 2221, 'bin': 2222, 'step': 2223, 'meeting': 2224, 'beers': 2225, 'paintings': 2226, 'interactive': 2227, 'lego': 2228, 'laughs': 2229, 's': 2230, 'controlled': 2231, 'check': 2232, 'hugging': 2233, 'keyboards': 2234, 'piano': 2235, 'cabinetry': 2236, 'angle': 2237, 'console': 2238, 'radio': 2239, 'recording': 2240, 'notebook': 2241, 'word': 2242, 'organized': 2243, 'magnets': 2244, 'leopard': 2245, 'castle': 2246, 'progress': 2247, 'workstation': 2248, 'bookshelves': 2249, 'today': 2250, 'learning': 2251, 'gaming': 2252, 'victorian': 2253, 'furnishings': 2254, 'fully': 2255, 'splash': 2256, 'checkered': 2257, 'unfinished': 2258, 'adorned': 2259, 'tripod': 2260, 'hockey': 2261, 'help': 2262, 'smartphone': 2263, 'wheelchair': 2264, 'major': 2265, 'sport': 2266, 'adjacent': 2267, 'soaring': 2268, 'cords': 2269, 'wireless': 2270, 'pouring': 2271, 'stylish': 2272, 'balcony': 2273, 'nothing': 2274, 'powered': 2275, 'daytime': 2276, 'lime': 2277, 'sofas': 2278, 'comfortable': 2279, 'first': 2280, 'unusual': 2281, 'blond': 2282, 'bathing': 2283, 'pottery': 2284, 'pub': 2285, 'opposing': 2286, 'desks': 2287, 'arched': 2288, 'bell': 2289, 'rundown': 2290, 'unit': 2291, 'us': 2292, 'lips': 2293, 'consisting': 2294, 'spilled': 2295, 'mess': 2296, 'because': 2297, 'atm': 2298, 'training': 2299, 'fans': 2300, 'carved': 2301, 'tulips': 2302, 'steak': 2303, 'parsley': 2304, 'peering': 2305, 'folding': 2306, 'guests': 2307, 'vines': 2308, 'opens': 2309, 'pickle': 2310, 'easy': 2311, 'halves': 2312, 'rise': 2313, 'designs': 2314, 'students': 2315, 'meats': 2316, 'strewn': 2317, 'student': 2318, 'quilt': 2319, 'meals': 2320, 'shrimp': 2321, 'patties': 2322, 'foam': 2323, 'finished': 2324, 'markers': 2325, 'marker': 2326, 'mexican': 2327, 'belly': 2328, 'dashboard': 2329, 'stew': 2330, 'tape': 2331, 'shelving': 2332, 'diners': 2333, 'ceramic': 2334, 'share': 2335, 'monkey': 2336, 'pancakes': 2337, 'sled': 2338, 'garlic': 2339, 'milkshake': 2340, 'styrofoam': 2341}\n"
     ]
    }
   ],
   "metadata": {
    "id": "pa_fIXj5_n6W"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "BATCH_SIZE = 4\n",
    "NUM_WORKER = 1\n",
    "#token to represent the padding\n",
    "pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKER,\n",
    "    shuffle=False,\n",
    "    collate_fn=new_collate\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "id": "e1ptid0sedWA"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Models"
   ],
   "metadata": {
    "id": "KXIFRNd6Rbih"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "def get_device(gpus=1):\n",
    "    if gpus==1:\n",
    "        return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    else:\n",
    "        if torch.cuda.is_available():\n",
    "            return f\"cuda:{gpus-1}\"\n",
    "        else:\n",
    "            return \"cpu\"\n",
    "\n",
    "device = get_device(1)\n",
    "\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, output_size, train_CNN=False):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.train_CNN = train_CNN\n",
    "        self.inception = models.inception_v3(pretrained=True, aux_logits=False)\n",
    "        self.inception.fc = nn.Linear(self.inception.fc.in_features, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, images):\n",
    "        '''\n",
    "        Input: image vector\n",
    "        Output: features vector\n",
    "        '''\n",
    "        features = self.inception(images)\n",
    "        output = self.relu(features)\n",
    "        return output\n",
    "\n",
    "class DecoderRNNV3(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, n_features):\n",
    "        '''\n",
    "        L- caption vec length\n",
    "        B- batch size\n",
    "        E- embed size\n",
    "        H- hidden size\n",
    "        F- number of features from CNN\n",
    "        '''\n",
    "        super(DecoderRNNV3, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = 3\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(input_size=embed_size+n_features,\n",
    "                            hidden_size=hidden_size, num_layers=self.num_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(in_features=hidden_size,\n",
    "                                out_features=vocab_size)\n",
    "\n",
    "    def forward(self, features, captions, cap_lengths):\n",
    "        # cap_lengths - list of the real length of each caption before padding\n",
    "        assert features.size(0) == captions.size(0)\n",
    "        # (h_0, c_0) will be initialized to zeros by default\n",
    "        # embed captions, shape (B, L, E)\n",
    "        captions_embed = self.embed(captions)\n",
    "        # features, shape (B, F)\n",
    "        # features transform shape to (B, L, F)\n",
    "        features = torch.unsqueeze(features, dim=1)  # (1,2048) -> (1,1,2048)\n",
    "        # (1,1,2048) -> (1,77, 2048)\n",
    "        features = features.repeat((1, captions_embed.size(1), 1))\n",
    "        # combine features + captions to shape (B, L, E+F) (1,77,2048) -> (1,77,2448)\n",
    "        combined = torch.cat((features, captions_embed), dim=2)\n",
    "        # create packedSequence that is better for LSTM\n",
    "        packed = pack_padded_sequence(\n",
    "            combined, cap_lengths, batch_first=True, enforce_sorted=False)\n",
    "        # run through the LSTM network and get output of shape (B, L, H)\n",
    "        lstm_out, _ = self.lstm(packed)\n",
    "        # unpack so we can use Linear function (works on Tensor not packSeq)\n",
    "        output_padded, output_lengths = pad_packed_sequence(\n",
    "            lstm_out, batch_first=True)\n",
    "\n",
    "        return self.fc_out(output_padded)\n",
    "\n",
    "    def caption_features(self, features, vocab, vec_len):\n",
    "        '''\n",
    "        Vec_len should be the same as is learning. \n",
    "        '''\n",
    "        assert features.size(\n",
    "            0) == 1, \"Caption features doesn't support batches\"\n",
    "        # features: (B,F) -> (1,1,F)\n",
    "        # w_embed: (1) -> (1,1,E)\n",
    "        w0 = torch.tensor(vocab.stoi[\"<SOS>\"]).to(device)\n",
    "        w0 = torch.unsqueeze(w0, 0)\n",
    "        w0 = torch.unsqueeze(w0, 0)\n",
    "        w_embed = self.embed(w0)\n",
    "        features = torch.unsqueeze(features, 1)\n",
    "        hi = torch.zeros((self.num_layers, 1, self.hidden_size)).to(device)\n",
    "        ci = torch.zeros((self.num_layers, 1, self.hidden_size)).to(device)\n",
    "        output = [\"<SOS>\"]\n",
    "        for i in range(vec_len):\n",
    "            combined = torch.cat((features, w_embed), dim=2)\n",
    "            if i == 0:\n",
    "                lstm_out, (hi, ci) = self.lstm(combined)\n",
    "            else:\n",
    "                lstm_out, (hi, ci) = self.lstm(combined, (hi, ci))\n",
    "            next_w = torch.argmax(self.fc_out(lstm_out), dim=2)\n",
    "            output.append(vocab.itos[next_w.item()])\n",
    "            # lstm_out: (1,1,F)\n",
    "            # hi, ci: (num_layers, 1, F)\n",
    "            # next_w: (1,1,vocab_size)\n",
    "            w_embed = self.embed(next_w)\n",
    "        return output\n",
    "\n",
    "    def sample(self, features, max_seq=50, states=None):\n",
    "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "        sampled_ids = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "        for i in range(max_seq):\n",
    "            hiddens, states = self.lstm(inputs, states)          # hiddens: (batch_size, 1, hidden_size)\n",
    "            outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
    "            _, predicted = outputs.max(1)                        # predicted: (batch_size)\n",
    "            sampled_ids.append(predicted)\n",
    "            inputs = self.embed(predicted)                       # inputs: (batch_size, embed_size)\n",
    "            inputs = inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)                # sampled_ids: (batch_size, max_seq_length)\n",
    "        return sampled_ids\n",
    "\n",
    "class CNNtoRNN(nn.Module):\n",
    "    def __init__(self, features, embed_size, hidden_size, vocab_size, train_CNN=False):\n",
    "        super(CNNtoRNN, self).__init__()\n",
    "        self.encoderCNN = EncoderCNN(features, train_CNN).to(device)\n",
    "        #self.decoderRNN = DecoderRNN(\n",
    "        #    embed_size, hidden_size, vocab_size).to(device)\n",
    "        self.decoderRNN = DecoderRNNV3(embed_size, hidden_size, vocab_size, features).to(device)\n",
    "\n",
    "    def forward(self, images, captions, cap_lengths, show=False):\n",
    "        features = self.encoderCNN(images)\n",
    "        outputs = self.decoderRNN(features, captions, cap_lengths)\n",
    "        return outputs\n",
    "\n",
    "    def caption_images(self, image, vocab, max_len=70):\n",
    "        # # Inference part\n",
    "        # # Given the image features generate the captions\n",
    "        # # input shape: (3,x,y) where, x,y: image size\n",
    "        # # ouput: captions list\n",
    "        # batch_size = image.size(0)\n",
    "        # assert batch_size == 1, \"Caption 1 image at a time\"\n",
    "        # image_pred = self.encoderCNN(image)\n",
    "\n",
    "        # # init the hidden and cell states to zeros\n",
    "        # hidden_state = torch.zeros((1, self.decoderRNN.hidden_size)).to(device)\n",
    "        # cell_state = torch.zeros((1, self.decoderRNN.hidden_size)).to(device)\n",
    "\n",
    "        # # starting input is\n",
    "        # captions = list()\n",
    "        # outputs = torch.empty(\n",
    "        #     (batch_size, max_len, self.decoderRNN.vocab_size)).to(device)\n",
    "        # hidden_state, cell_state = self.decoderRNN.lstm_cell(\n",
    "        #     image_pred, (hidden_state, cell_state))\n",
    "        # out = self.decoderRNN.fc_out(self.decoderRNN.dropout(hidden_state))\n",
    "        # word_embed = self.decoderRNN.embed(torch.tensor(\n",
    "        #     vocab.stoi[\"<SOS>\"]).to(device)).unsqueeze(0)\n",
    "        # for t in range(max_len):\n",
    "        #     # for the first time step the input is the feature vector\n",
    "        #     # if t == 0:\n",
    "        #     #     hidden_state, cell_state = self.decoderRNN.lstm_cell(image_pred, (hidden_state, cell_state))\n",
    "        #     # for the 2nd+ time step, use previously generated caption\n",
    "        #     # else:\n",
    "        #     hidden_state, cell_state = self.decoderRNN.lstm_cell(\n",
    "        #         word_embed, (hidden_state, cell_state))\n",
    "\n",
    "        #     # output of the attention mechanism\n",
    "        #     out = self.decoderRNN.fc_out(hidden_state)\n",
    "        #     outputs[:, t, :] = out\n",
    "        #     #print(f\"out shape:{out.shape}\")\n",
    "        #     captions.append(torch.argmax(out, dim=1))\n",
    "        #     #print(f\"\\n predicted outputs:{captions}\")\n",
    "        #     word_embed = self.decoderRNN.embed(\n",
    "        #         torch.argmax(out[0])).unsqueeze(0)\n",
    "        #     last_word_idx = captions[-1].item()\n",
    "        #     if vocab.itos[last_word_idx] == vocab.stoi[\"<EOS>\"]:\n",
    "        #         print(\"BREAKING!!!!!!\")\n",
    "        #         break\n",
    "\n",
    "        #         # build the output tensor\n",
    "        #     # print(captions)\n",
    "        #     # covert the vocab idx to words and return sentence\n",
    "        # print(f\"outputs shape:{outputs.shape}\")\n",
    "        # print(f\"Outputs argmax dim2:{torch.argmax(outputs,dim=2)}\")\n",
    "        # return [vocab.itos[idx.item()] for idx in captions if idx.item() != vocab.stoi[\"<PAD>\"]]\n",
    "        features = self.encoderCNN(image)\n",
    "        decoded = self.decoderRNN.sample(features, max_len)\n",
    "        return decoded\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "kxeR_Dg0JnNQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "id": "yTBnd6f5RgQd"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## training function"
   ],
   "metadata": {
    "id": "5qTIQN7TWqOJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def show_image(img, title=None, transform=True, f_name=\"\"):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    # unnormalize\n",
    "    if transform:\n",
    "        img[0] = img[0] * 0.229\n",
    "        img[1] = img[1] * 0.224\n",
    "        img[2] = img[2] * 0.225\n",
    "        img[0] += 0.485\n",
    "        img[1] += 0.456\n",
    "        img[2] += 0.406\n",
    "\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    # title = title.replace(\"<SOS>\",\"\").replace(\"<EOS>\", \"\")\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.imsave(f'{f_name.replace(\".png\", \"\")}_{title}.png', img)\n",
    "    print(f'Saved {f_name} with caption {plt.title}')\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    \n",
    "\n",
    "\n",
    "def train(max_epochs: int, model, progress=250):\n",
    "    \"\"\"\n",
    "    Train a given model\n",
    "    Args:\n",
    "        max_epochs (int): Number of epoches to train on\n",
    "        model ([type]): Model to train\n",
    "        data_loader ([type]): Dataloader\n",
    "        device (str): CPU or GPU\n",
    "        progress (int, optional): Show prediction and loss values every X iterations. Defaults to 250.\n",
    "\n",
    "    Returns:\n",
    "        [type]: Trained model\n",
    "    \"\"\"\n",
    "    # Hyperparameters\n",
    "    learning_rate = 3e-4\n",
    "    # init model\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "\n",
    "    # start epochs\n",
    "    for epoch in range(max_epochs):\n",
    "        for idx, (img, captions, length) in tqdm(\n",
    "            enumerate(data_loader), total=len(data_loader), leave=False\n",
    "        ):\n",
    "            img = img.to(device)\n",
    "            captions = captions.to(device).long()\n",
    "            output = model(img, captions, length)\n",
    "            loss = criterion(\n",
    "                output.reshape(-1, output.shape[2]), captions.reshape(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(loss)\n",
    "            optimizer.step()\n",
    "            if idx > 0 and idx % 100 == 0:\n",
    "                torch.save({'model_state_dict': model.state_dict()}, \"checkpoint.torch\")\n",
    "                            \n",
    "                dataiter = iter(data_loader)\n",
    "                img_show, cap, cap_len = next(dataiter)\n",
    "                output = model(img_show.to(device),\n",
    "                               cap.to(device).long(), cap_len).to(device)\n",
    "                print(f\"\\n\\nLoss {loss.item():.5f}\\n\")\n",
    "                print(f\"\\nForward\\n\")\n",
    "                out_cap = torch.argmax(output[0], dim=1)\n",
    "                demo_cap = ' '.join([data_loader.dataset.vocab.itos[idx2.item(\n",
    "                )] for idx2 in out_cap if idx2.item() != data_loader.dataset.vocab.stoi[\"<PAD>\"]])\n",
    "                # show_image(show_img[0], title=demo_cap, f_name=\"Forward.png\")\n",
    "                print(demo_cap)\n",
    "                demo_cap = model.caption_images(img_show[0:1].to(\n",
    "                    device), vocab=data_loader.dataset.vocab, max_len=30)\n",
    "                demo_cap = ' '.join(demo_cap)\n",
    "                print(\"Predicted\")\n",
    "                print(demo_cap)\n",
    "                # show_image(img_show[0], title=demo_cap, f_name=\"Predicted.png\")\n",
    "                print(\"Original\")\n",
    "                cap = cap[0]\n",
    "                # print(cap.long())\n",
    "                demo_cap = ' '.join([data_loader.dataset.vocab.itos[idx2.item(\n",
    "                )] for idx2 in cap if idx2.item() != data_loader.dataset.vocab.stoi[\"<PAD>\"]])\n",
    "                print(demo_cap)\n",
    "                # show_image(img_show[0], title=demo_cap, transform=False, f_name=\"Original.png\")\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {
    "id": "Yu_HfkQyRiAP"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## image function"
   ],
   "metadata": {
    "id": "4CWQGhuOWvF9"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_image(img, title=None, transform=True, f_name=''):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    \n",
    "    #unnormalize \n",
    "    if transform:\n",
    "      img[0] = img[0] * 0.229\n",
    "      img[1] = img[1] * 0.224 \n",
    "      img[2] = img[2] * 0.225 \n",
    "      img[0] += 0.485 \n",
    "      img[1] += 0.456 \n",
    "      img[2] += 0.406\n",
    "      \n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    \n",
    "    plt.imshow(img)\n",
    "    # title = title.replace(\"<SOS>\",\"\").replace(\"<EOS>\", \"\")\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ],
   "outputs": [],
   "metadata": {
    "id": "a1APdbNyWw8R"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overfit sanity check"
   ],
   "metadata": {
    "id": "KceQyQ0uqiRM"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "tqdm = partial(tqdm, position=0, leave=True)\n",
    "def overfit(model, T=250):\n",
    "    \"\"\"\n",
    "    Run a training on one image+caption\n",
    "    Args:\n",
    "        model ([type]): Model to train\n",
    "        device ([type]): CPU or GPU\n",
    "        data_loader ([type]): Dataloader\n",
    "        T (int, optional): How many iterations to run training for. Defaults to 250.\n",
    "    \"\"\"\n",
    "    tqdm_bar = partial(tqdm, position=0, leave=True)\n",
    "\n",
    "    learning_rate = 3e-4\n",
    "\n",
    "\n",
    "    # init model\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    dataiter = iter(data_loader)\n",
    "    img, caption, length = next(dataiter)\n",
    "    for i in tqdm_bar(range(T)):\n",
    "        # train on the same image and caption to achieve overfitting\n",
    "        img = img.to(device)\n",
    "        caption = caption.to(device).long()\n",
    "        output = model(img, caption, length).to(device)\n",
    "        loss = criterion(\n",
    "            output.reshape(-1, output.shape[2]), caption.reshape(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "    output = model(img, caption, length).to(device)\n",
    "    show_img = img.to(\"cpu\")\n",
    "    print(f\"\\n\\nLoss {loss.item():.5f}\\n\")\n",
    "    out_cap = torch.argmax(output[0], dim=1)\n",
    "    demo_cap = ' '.join([data_loader.dataset.vocab.itos[idx2.item(\n",
    "    )] for idx2 in out_cap if idx2.item() != data_loader.dataset.vocab.stoi[\"<PAD>\"]])\n",
    "    show_image(show_img[0], title=demo_cap, f_name=\"Forward.png\")\n",
    "    print(\"Predicted\")\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        demo_cap = model.caption_images(show_img[0:1].to(\n",
    "            device), vocab=data_loader.dataset.vocab, max_len=15)\n",
    "        demo_cap = ' '.join(demo_cap)\n",
    "        model.train()\n",
    "\n",
    "        show_image(show_img[0], title=demo_cap,\n",
    "                   transform=False, f_name=\"Predicted.png\")\n",
    "    print(\"Original\")\n",
    "    cap = caption[0]\n",
    "    # print(cap.long())\n",
    "    demo_cap = ' '.join([data_loader.dataset.vocab.itos[idx2.item(\n",
    "    )] for idx2 in cap if idx2.item() != data_loader.dataset.vocab.stoi[\"<PAD>\"]])\n",
    "    show_image(show_img[0], title=demo_cap,\n",
    "               transform=False, f_name=\"Original.png\")"
   ],
   "outputs": [],
   "metadata": {
    "id": "JcQJ733Nql1V"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Process"
   ],
   "metadata": {
    "id": "rABCvI85Wx_9"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "embed_size = 256\n",
    "hidden_size = 256\n",
    "vocab_size = len(dataset.vocab)\n",
    "model = CNNtoRNN(2304, embed_size, hidden_size, vocab_size, train_CNN=False)\n",
    "trained_model = train(3, model)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "                                                 "
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "Loss 5.55838\n",
      "\n",
      "\n",
      "Forward\n",
      "\n",
      "<SOS> a a a a a a <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 2560, got 2304",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-1492fc8baefc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNNtoRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2304\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_CNN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-9c8df7286317>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(max_epochs, model, progress)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;31m# show_image(show_img[0], title=demo_cap, f_name=\"Forward.png\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdemo_cap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 demo_cap = model.caption_images(img_show[0:1].to(\n\u001b[0m\u001b[1;32m     75\u001b[0m                     device), vocab=data_loader.dataset.vocab, max_len=30)\n\u001b[1;32m     76\u001b[0m                 \u001b[0mdemo_cap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdemo_cap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-2dcc5e175fdd>\u001b[0m in \u001b[0;36mcaption_images\u001b[0;34m(self, image, vocab, max_len)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;31m# return [vocab.itos[idx.item()] for idx in captions if idx.item() != vocab.stoi[\"<PAD>\"]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoderCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoderRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-2dcc5e175fdd>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, features, max_seq, states)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mhiddens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# hiddens: (batch_size, 1, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhiddens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;31m# outputs:  (batch_size, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m                        \u001b[0;31m# predicted: (batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yandex/DLW2021/pelegv/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yandex/DLW2021/pelegv/anaconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[0;32m/home/yandex/DLW2021/pelegv/anaconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    618\u001b[0m                            \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                            ):\n\u001b[0;32m--> 620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         self.check_hidden_size(hidden[0], self.get_expected_hidden_size(input, batch_sizes),\n\u001b[1;32m    622\u001b[0m                                'Expected hidden[0] size {}, got {}')\n",
      "\u001b[0;32m/home/yandex/DLW2021/pelegv/anaconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    203\u001b[0m                     expected_input_dim, input.dim()))\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    206\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[1;32m    207\u001b[0m                     self.input_size, input.size(-1)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 2560, got 2304"
     ]
    }
   ],
   "metadata": {
    "id": "z6aG4SBFV7UM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Misc"
   ],
   "metadata": {
    "id": "2Eqf6B56tjS1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "embed_size = 256\n",
    "hidden_size = 256\n",
    "vocab_size = len(dataset.vocab)\n",
    "model = CNNtoRNN(2048,embed_size, hidden_size, vocab_size, train_CNN=False)\n",
    "overfit(model, 500)\n",
    "del model"
   ],
   "outputs": [],
   "metadata": {
    "id": "QQmoP_Ltd9pM"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for imgs,caps in data_loader:\n",
    "    print(f\"\\nimg shape:{imgs.shape}\")\n",
    "    print(f\"\\ncap shape:{caps.shape}\")\n",
    "    print(f\"\\nFirst caption:{caps[0]}\")\n",
    "    print(f\"\\nFirst embed:{caps[0][0]}\")\n",
    "    break"
   ],
   "outputs": [],
   "metadata": {
    "id": "WtuAGKC3gftz"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!conda install -c conda-forge -y ipywidgets "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!source /home/yandex/DLW2021/davidhay/anaconda3/bin/activate\n",
    "!conda info"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!conda activate /home/yandex/DLW2021/davidhay/anaconda3/envs/new-env\n",
    "!conda info --envs\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_allocated()\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import gc\n",
    "import torch\n",
    "def memReport():\n",
    "    for obj in gc.get_objects():\n",
    "        if torch.is_tensor(obj):\n",
    "            print(type(obj), obj.size())\n",
    "gc.collect()\n",
    "memReport()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/yandex/DLW2021/davidhay/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:151: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "!nvidia-smi\n",
    "!kill -9 40485"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sat Aug 28 11:43:55 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.27.04    Driver Version: 460.27.04    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN Xp            On   | 00000000:3D:00.0 Off |                  N/A |\n",
      "| 33%   55C    P2   119W / 250W |   5061MiB / 12196MiB |     70%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  TITAN Xp            On   | 00000000:3E:00.0 Off |                  N/A |\n",
      "| 23%   21C    P8     9W / 250W |      1MiB / 12196MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  TITAN Xp            On   | 00000000:60:00.0 Off |                  N/A |\n",
      "| 23%   22C    P8     8W / 250W |      1MiB / 12196MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  TITAN Xp            On   | 00000000:61:00.0 Off |                  N/A |\n",
      "| 23%   24C    P8     8W / 250W |      1MiB / 12196MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  TITAN Xp            On   | 00000000:B1:00.0 Off |                  N/A |\n",
      "| 23%   22C    P8     8W / 250W |      1MiB / 12196MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  TITAN Xp            On   | 00000000:B2:00.0 Off |                  N/A |\n",
      "| 23%   23C    P8     8W / 250W |      1MiB / 12196MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  TITAN Xp            On   | 00000000:DA:00.0 Off |                  N/A |\n",
      "| 23%   21C    P8     8W / 250W |      1MiB / 12196MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  TITAN Xp            On   | 00000000:DB:00.0 Off |                  N/A |\n",
      "| 23%   20C    P8     8W / 250W |      1MiB / 12196MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     40485      C   python3                          5057MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "40485: Operation not permitted\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_loader.dataset"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "image captioning deep learning workshop.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "conda-root-py",
   "display_name": "Python [conda env:root] *",
   "language": "python"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}