{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Downloads"
   ],
   "metadata": {
    "id": "PJ_DSjOA83Oe"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#%cd /home/yandex/DLW2021/davidhay/\n",
    "#!pwd\n",
    "#!ls -l "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#!rm -rf coco\n",
    "#!mkdir coco\n",
    "#%cd coco"
   ],
   "outputs": [],
   "metadata": {
    "id": "IkLieRxMxqdZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#!wget -c http://images.cocodataset.org/zips/train2017.zip\n",
    "#!unzip train2017.zip\n",
    "#!rm train2017.zip\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#!wget -c http://images.cocodataset.org/zips/val2017.zip\n",
    "\n",
    "#!unzip val2017.zip\n",
    "\n",
    "#!rm val2017.zip\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\n",
    "#!wget -c http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "\n",
    "#!unzip annotations_trainval2017.zip\n",
    "\n",
    "#!rm annotations_trainval2017.zip"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Loading"
   ],
   "metadata": {
    "id": "FEZMYzlIxCwV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports and Vocabulary "
   ],
   "metadata": {
    "id": "pnc6rR2P-ck-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "\n",
    "import os\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self,freq_threshold):\n",
    "        #setting the pre-reserved tokens int to string tokens\n",
    "        # PAD- padding symbol\n",
    "        # SOS- Start of Sentence\n",
    "        # EOS- end of sentence\n",
    "        # UNK- unknown word (unknown\\ below threshold)\n",
    "        self.itos = {0:\"<PAD>\",1:\"<SOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n",
    "        #string to int tokens\n",
    "        #its reverse dict self.itos\n",
    "        self.stoi = {v:k for k,v in self.itos.items()}\n",
    "        self.freq_threshold = freq_threshold\n",
    "        \n",
    "    def __len__(self):\n",
    "      return len(self.itos)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        return [token.text.lower() for token in spacy_eng.tokenizer(text)]\n",
    "    \n",
    "    def build_vocab(self, sentence_list):\n",
    "        frequencies = Counter()\n",
    "        idx = 4\n",
    "        for index,sentence in enumerate(sentence_list):\n",
    "\n",
    "            for word in self.tokenize(sentence):\n",
    "                frequencies[word] += 1\n",
    "                \n",
    "                #add the word to the vocab if it reaches minum frequecy threshold\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    # if idx > 0 and idx % 1000==0:\n",
    "                        # print(f\"Added {idx} words to vocab\")\n",
    "                    idx += 1\n",
    "            # if index>0 and index%1000==0:\n",
    "                # print(f\"Iterated {index} sentences\")\n",
    "            if len(self.stoi) >= 5000:\n",
    "                break\n",
    "             \n",
    "\n",
    "        print(f\"Done, added {idx-1} words to vocabulary\")\n",
    "    \n",
    "    def numericalize(self,text):\n",
    "        \"\"\" For each word in the text corresponding index token for that word form the vocab built as list \"\"\"\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        result = [ self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text ]\n",
    "        return result"
   ],
   "outputs": [],
   "metadata": {
    "id": "6YUFskmKSJlD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset custom class"
   ],
   "metadata": {
    "id": "rkgLLalGAHXn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pickle\n",
    "class COCODataset(Dataset):\n",
    "    \"\"\"\n",
    "    COCODataset\n",
    "    \"\"\"\n",
    "    def __init__(self,root_dir,annotation_file,transform=None,freq_threshold=5,\n",
    "                 load_vocab=False, vocab_loc = \"vocab.pkl\"):\n",
    "      \"\"\"\n",
    "      can use load_vocab to use a previously created vocabulary (time saving feature)\n",
    "      freq_threshold: words with a count below this number will be marked as <UNK>\n",
    "      \"\"\"\n",
    "      self.root_dir = root_dir\n",
    "      self.coco = COCO(annotation_file)\n",
    "      self.transform = transform\n",
    "      self.cap_max_size = 0\n",
    "      #Get image and caption colum from the dataframe\n",
    "      self.imgs = []\n",
    "      self.captions = []\n",
    "      for idx,ann in enumerate(self.coco.anns.values()):\n",
    "        self.imgs.append(self.coco.loadImgs((ann['image_id']))[0][\"file_name\"])\n",
    "        self.captions.append(ann['caption'])\n",
    "        #if (idx) % 1000 == 0 and idx>0:\n",
    "          #print(f\"Processed {idx} images and captions\")\n",
    "      print(\"Finished processing images and captions\")\n",
    "      print(f\"Got:{len(set(self.imgs))} pictures with {len(self.captions)} captions!\")\n",
    "      \n",
    "      #Initialize vocabulary and build vocab\n",
    "      if load_vocab:\n",
    "        with open(vocab_loc, \"rb\") as source:\n",
    "          self.vocab = pickle.load(source)\n",
    "        print(f\"Loaded vocabulary from {vocab_loc}\")\n",
    "      \n",
    "      else:\n",
    "        print(\"Build vocabulary\")\n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocab(self.captions)\n",
    "        print(\"Finished building vocabulary\")\n",
    "        with open(vocab_loc, \"wb\") as dest:\n",
    "          pickle.dump(self.vocab, dest)\n",
    "      \n",
    "      print(f\"Using {len(self.vocab)} words\")\n",
    "    \n",
    "    def __len__(self):\n",
    "      return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "      caption = self.captions[idx]\n",
    "      img_name = self.imgs[idx]\n",
    "      img_location = os.path.join(self.root_dir,img_name)\n",
    "      img = Image.open(img_location).convert(\"RGB\")\n",
    "      \n",
    "      #apply the transfromation to the image\n",
    "      if self.transform:\n",
    "          img = self.transform(img)\n",
    "      \n",
    "      #numericalize the caption text\n",
    "      caption_vec = [self.vocab.stoi[\"<SOS>\"]]\n",
    "      caption_vec.extend(self.vocab.numericalize(caption))\n",
    "      caption_vec.append(self.vocab.stoi[\"<EOS>\"])\n",
    "      \n",
    "      return img, torch.tensor(caption_vec,dtype=torch.long)"
   ],
   "outputs": [],
   "metadata": {
    "id": "Y19oWTAsWmEC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataloader creation"
   ],
   "metadata": {
    "id": "wDqCBk-KCBkw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# define a transformation to add some noise and variance to our images\n",
    "transformation = transforms.Compose([transforms.Resize((299,299), Image.NEAREST),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.RandomVerticalFlip(),\n",
    "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                                      ])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/yandex/DLW2021/pelegv/anaconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py:280: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "metadata": {
    "id": "pNHTCZ8ZcXGQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "def new_collate(batch):\n",
    "    (imgs, targets) = zip(*batch)\n",
    "    imgs = [x.unsqueeze(0) for x in imgs]\n",
    "    imgs = torch.cat(imgs,dim=0)\n",
    "    targets_lens = [len(target) for target in targets]\n",
    "    targets_pad = pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "\n",
    "    return imgs, targets_pad, targets_lens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\n",
    "dataset =  COCODataset(\n",
    "    root_dir = \"/home/yandex/DLW2021/davidhay/coco/train2017\",\n",
    "    annotation_file= \"/home/yandex/DLW2021/davidhay/coco/annotations/captions_train2017.json\",\n",
    "    transform=transformation,\n",
    "    freq_threshold=5,\n",
    "    load_vocab=False\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=4.13s)\n",
      "creating index...\n",
      "index created!\n",
      "Finished processing images and captions\n",
      "Got:118287 pictures with 591753 captions!\n",
      "Build vocabulary\n",
      "Done, added 4999 words to vocabulary\n",
      "Finished building vocabulary\n",
      "Using 5000 words\n"
     ]
    }
   ],
   "metadata": {
    "id": "igD_-Qs7oFTz"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "BATCH_SIZE = 4\n",
    "NUM_WORKER = 1\n",
    "#token to represent the padding\n",
    "pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKER,\n",
    "    shuffle=False,\n",
    "    collate_fn=new_collate\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "id": "e1ptid0sedWA"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Models"
   ],
   "metadata": {
    "id": "KXIFRNd6Rbih"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "def get_device(gpus=1):\n",
    "    if gpus==1:\n",
    "        return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    else:\n",
    "        if torch.cuda.is_available():\n",
    "            return f\"cuda:{gpus-1}\"\n",
    "        else:\n",
    "            return \"cpu\"\n",
    "\n",
    "device = get_device(1)\n",
    "\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size, train_CNN=False):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.train_CNN = train_CNN\n",
    "        self.inception = models.inception_v3(pretrained=True, aux_logits=False)\n",
    "        # make the features tensor in the embed size length\n",
    "        self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size) \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, images):\n",
    "        '''\n",
    "        Input: image vector\n",
    "        Output: features vector\n",
    "        '''\n",
    "        features = self.inception(images)\n",
    "        # print(\"features size: \", features.size())\n",
    "        output = self.relu(features)\n",
    "        # print(\"output size: \", output.size())\n",
    "        return output\n",
    "\n",
    "\n",
    "class DecoderRNNV4(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=3):\n",
    "        super(DecoderRNNV4, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(in_features=hidden_size, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, features, captions, cap_lengths):\n",
    "        # cap_lengths - list of the real length of each caption before padding\n",
    "        assert features.size(0) == captions.size(0)\n",
    "        \n",
    "        # embed captions, shape (B, L, E)\n",
    "        captions_embed = self.embed(captions)\n",
    "        # features, shape (B, E)\n",
    "        # features transform shape to (B, L, E)\n",
    "        features = torch.unsqueeze(features, dim=1)  # (1,256) -> (1,1,256)\n",
    "        \n",
    "        # (1,1,256) -> (1,77, 256)\n",
    "        # features = features.repeat((1, captions_embed.size(1), 1))\n",
    "        # print(\"features size 2:\", features.size())\n",
    "        # combine features + captions to shape (B, 1+L, E) (1,1,256) -> (1,14,256)\n",
    "        combined = torch.cat((features, captions_embed), dim=1)\n",
    "        \n",
    "        # create packedSequence that is better for LSTM\n",
    "        packed = pack_padded_sequence(\n",
    "            combined, cap_lengths, batch_first=True, enforce_sorted=False)\n",
    "        # run through the LSTM network and get output of shape (B, L, H)\n",
    "        lstm_out, _ = self.lstm(packed)\n",
    "        # unpack so we can use Linear function (works on Tensor not packSeq)\n",
    "        output_padded, output_lengths = pad_packed_sequence(\n",
    "            lstm_out, batch_first=True)\n",
    "\n",
    "        return self.fc_out(output_padded)\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "class CNNtoRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, train_CNN=False):\n",
    "        super(CNNtoRNN, self).__init__()\n",
    "        self.encoderCNN = EncoderCNN(embed_size, train_CNN).to(device)\n",
    "        \n",
    "        # self.decoderRNN = DecoderRNNV3(embed_size, hidden_size, vocab_size, features).to(device)\n",
    "        self.decoderRNN = DecoderRNNV4(embed_size, hidden_size, vocab_size).to(device)\n",
    "\n",
    "    def forward(self, images, captions, cap_lengths, show=False):\n",
    "        features = self.encoderCNN(images)\n",
    "        outputs = self.decoderRNN(features, captions, cap_lengths)\n",
    "        return outputs\n",
    "\n",
    "    def caption_images(self, features, vocab, max_len=77):\n",
    "        '''\n",
    "        Vec_len should be the same as is learning. \n",
    "        '''\n",
    "        assert features.size(\n",
    "            0) == 1, f\"Caption features doesn't support batches got {features.shape}\"\n",
    "        # features: (B,F) -> (1,1,F)\n",
    "        # w_embed: (1) -> (1,1,E)\n",
    "        result_caption = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x = self.encoderCNN(features).unsqueeze(0)\n",
    "            states = None\n",
    "            x = self.decoderRNN.fc_in(x)\n",
    "            for _ in range(max_len):\n",
    "                hiddens, states = self.decoderRNN.lstm(x, states)\n",
    "                output = self.decoderRNN.fc_out(hiddens.squeeze(0))\n",
    "                predicted = output.argmax(1)\n",
    "                result_caption.append(predicted.item())\n",
    "                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n",
    "\n",
    "                if vocab.itos[predicted.item()] == \"<EOS>\":\n",
    "                    break\n",
    "\n",
    "        return [vocab.itos[idx] for idx in result_caption]\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        super().train(mode)\n",
    "        if not self.encoderCNN.train_CNN:\n",
    "            self.encoderCNN.eval()\n",
    "        return self"
   ],
   "outputs": [],
   "metadata": {
    "id": "kxeR_Dg0JnNQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "id": "yTBnd6f5RgQd"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## training function"
   ],
   "metadata": {
    "id": "5qTIQN7TWqOJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def show_image(img, title=None, transform=True, f_name=\"\"):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    # unnormalize\n",
    "    if transform:\n",
    "        img[0] = img[0] * 0.229\n",
    "        img[1] = img[1] * 0.224\n",
    "        img[2] = img[2] * 0.225\n",
    "        img[0] += 0.485\n",
    "        img[1] += 0.456\n",
    "        img[2] += 0.406\n",
    "\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    # title = title.replace(\"<SOS>\",\"\").replace(\"<EOS>\", \"\")\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.imsave(f'{f_name.replace(\".png\", \"\")}_{title}.png', img)\n",
    "    print(f'Saved {f_name} with caption {plt.title}')\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    \n",
    "\n",
    "\n",
    "def train(max_epochs: int, model, progress=250):\n",
    "    \"\"\"\n",
    "    Train a given model\n",
    "    Args:\n",
    "        max_epochs (int): Number of epoches to train on\n",
    "        model ([type]): Model to train\n",
    "        data_loader ([type]): Dataloader\n",
    "        device (str): CPU or GPU\n",
    "        progress (int, optional): Show prediction and loss values every X iterations. Defaults to 250.\n",
    "\n",
    "    Returns:\n",
    "        [type]: Trained model\n",
    "    \"\"\"\n",
    "    # Hyperparameters\n",
    "    learning_rate = 3e-4\n",
    "    # init model\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "\n",
    "    # start epochs\n",
    "    for epoch in range(max_epochs):\n",
    "        for idx, (img, captions, length) in tqdm(\n",
    "            enumerate(data_loader), total=len(data_loader), leave=False\n",
    "        ):\n",
    "            img = img.to(device)\n",
    "            captions = captions.to(device).long()\n",
    "            output = model(img, captions, length)\n",
    "            loss = criterion(\n",
    "                output.reshape(-1, output.shape[2]), captions.reshape(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(loss)\n",
    "            optimizer.step()\n",
    "            if idx > 0 and idx % 3000 == 0:\n",
    "                torch.save({'model_state_dict': model.state_dict()}, \"checkpoint.torch\")\n",
    "                            \n",
    "                dataiter = iter(data_loader)\n",
    "                img_show, cap, cap_len = next(dataiter)\n",
    "                output = model(img_show.to(device),\n",
    "                               cap.to(device).long(), cap_len).to(device)\n",
    "                print(f\"\\n\\nLoss {loss.item():.5f}\\n\")\n",
    "                print(f\"\\nForward\\n\")\n",
    "                out_cap = torch.argmax(output[0], dim=1)\n",
    "                demo_cap = ' '.join([data_loader.dataset.vocab.itos[idx2.item(\n",
    "                )] for idx2 in out_cap if idx2.item() != data_loader.dataset.vocab.stoi[\"<PAD>\"]])\n",
    "                # show_image(show_img[0], title=demo_cap, f_name=\"Forward.png\")\n",
    "                print(demo_cap)\n",
    "                demo_cap = model.sample(img_show[0:1].to(\n",
    "                    device), vocabulary=data_loader.dataset.vocab)\n",
    "                demo_cap = ' '.join(demo_cap)\n",
    "                print(\"Predicted\")\n",
    "                print(demo_cap)\n",
    "                # show_image(img_show[0], title=demo_cap, f_name=\"Predicted.png\")\n",
    "                print(\"Original\")\n",
    "                cap = cap[0]\n",
    "                # print(cap.long())\n",
    "                demo_cap = ' '.join([data_loader.dataset.vocab.itos[idx2.item(\n",
    "                )] for idx2 in cap if idx2.item() != data_loader.dataset.vocab.stoi[\"<PAD>\"]])\n",
    "                print(demo_cap)\n",
    "                # show_image(img_show[0], title=demo_cap, transform=False, f_name=\"Original.png\")\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {
    "id": "Yu_HfkQyRiAP"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## image function"
   ],
   "metadata": {
    "id": "4CWQGhuOWvF9"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_image(img, title=None, transform=True, f_name=''):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    \n",
    "    #unnormalize \n",
    "    if transform:\n",
    "      img[0] = img[0] * 0.229\n",
    "      img[1] = img[1] * 0.224 \n",
    "      img[2] = img[2] * 0.225 \n",
    "      img[0] += 0.485 \n",
    "      img[1] += 0.456 \n",
    "      img[2] += 0.406\n",
    "      \n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    \n",
    "    plt.imshow(img)\n",
    "    # title = title.replace(\"<SOS>\",\"\").replace(\"<EOS>\", \"\")\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ],
   "outputs": [],
   "metadata": {
    "id": "a1APdbNyWw8R"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overfit sanity check"
   ],
   "metadata": {
    "id": "KceQyQ0uqiRM"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "tqdm = partial(tqdm, position=0, leave=True)\n",
    "def overfit(model, T=250):\n",
    "    \"\"\"\n",
    "    Run a training on one image+caption\n",
    "    Args:\n",
    "        model ([type]): Model to train\n",
    "        device ([type]): CPU or GPU\n",
    "        data_loader ([type]): Dataloader\n",
    "        T (int, optional): How many iterations to run training for. Defaults to 250.\n",
    "    \"\"\"\n",
    "    tqdm_bar = partial(tqdm, position=0, leave=True)\n",
    "\n",
    "    learning_rate = 3e-4\n",
    "\n",
    "\n",
    "    # init model\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    dataiter = iter(data_loader)\n",
    "    img, caption, length = next(dataiter)\n",
    "    for i in tqdm_bar(range(T)):\n",
    "        # train on the same image and caption to achieve overfitting\n",
    "        img = img.to(device)\n",
    "        caption = caption.to(device).long()\n",
    "        output = model(img, caption, length).to(device)\n",
    "        loss = criterion(\n",
    "            output.reshape(-1, output.shape[2]), caption.reshape(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "    output = model(img, caption, length).to(device)\n",
    "    show_img = img.to(\"cpu\")\n",
    "    print(f\"\\n\\nLoss {loss.item():.5f}\\n\")\n",
    "    out_cap = torch.argmax(output[0], dim=1)\n",
    "    demo_cap = ' '.join([data_loader.dataset.vocab.itos[idx2.item(\n",
    "    )] for idx2 in out_cap if idx2.item() != data_loader.dataset.vocab.stoi[\"<PAD>\"]])\n",
    "    show_image(show_img[0], title=demo_cap, f_name=\"Forward.png\")\n",
    "    print(\"Predicted\")\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        demo_cap = model.sample(show_img[0:1].to(\n",
    "            device), vocabulary=data_loader.dataset.vocab, max_length=15)\n",
    "        demo_cap = ' '.join(demo_cap)\n",
    "        model.train()\n",
    "\n",
    "        show_image(show_img[0], title=demo_cap,\n",
    "                   transform=False, f_name=\"Predicted.png\")\n",
    "    print(\"Original\")\n",
    "    cap = caption[0]\n",
    "    # print(cap.long())\n",
    "    demo_cap = ' '.join([data_loader.dataset.vocab.itos[idx2.item(\n",
    "    )] for idx2 in cap if idx2.item() != data_loader.dataset.vocab.stoi[\"<PAD>\"]])\n",
    "    show_image(show_img[0], title=demo_cap,\n",
    "               transform=False, f_name=\"Original.png\")"
   ],
   "outputs": [],
   "metadata": {
    "id": "JcQJ733Nql1V"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Process"
   ],
   "metadata": {
    "id": "rABCvI85Wx_9"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "embed_size = 2048\n",
    "hidden_size = 512\n",
    "vocab_size = len(dataset.vocab)\n",
    "model = CNNtoRNN(embed_size, hidden_size, vocab_size, train_CNN=False)\n",
    "trained_model = train(5, model)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 24%|██▍       | 1487/6254 [01:50<05:50, 13.61it/s]"
     ]
    }
   ],
   "metadata": {
    "id": "z6aG4SBFV7UM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Misc"
   ],
   "metadata": {
    "id": "2Eqf6B56tjS1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "embed_size = 1024\n",
    "hidden_size = 512\n",
    "vocab_size = len(dataset.vocab)\n",
    "model = CNNtoRNN(embed_size, hidden_size, vocab_size, train_CNN=False)\n",
    "overfit(model, 200)\n",
    "del model"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/yandex/DLW2021/pelegv/anaconda3/lib/python3.8/site-packages/torch/cuda/__init__.py:106: UserWarning: \n",
      "GeForce RTX 3090 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 compute_37.\n",
      "If you want to use the GeForce RTX 3090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    }
   ],
   "metadata": {
    "id": "QQmoP_Ltd9pM"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for imgs,caps in data_loader:\n",
    "    print(f\"\\nimg shape:{imgs.shape}\")\n",
    "    print(f\"\\ncap shape:{caps.shape}\")\n",
    "    print(f\"\\nFirst caption:{caps[0]}\")\n",
    "    print(f\"\\nFirst embed:{caps[0][0]}\")\n",
    "    break"
   ],
   "outputs": [],
   "metadata": {
    "id": "WtuAGKC3gftz"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!conda install -c conda-forge -y ipywidgets "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!source /home/yandex/DLW2021/davidhay/anaconda3/bin/activate\n",
    "!conda info"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!conda activate /home/yandex/DLW2021/davidhay/anaconda3/envs/new-env\n",
    "!conda info --envs\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_allocated()\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import gc\n",
    "import torch\n",
    "def memReport():\n",
    "    for obj in gc.get_objects():\n",
    "        if torch.is_tensor(obj):\n",
    "            print(type(obj), obj.size())\n",
    "gc.collect()\n",
    "memReport()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/yandex/DLW2021/davidhay/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:151: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "!nvidia-smi\n",
    "!kill -9 40485"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sat Aug 28 11:43:55 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.27.04    Driver Version: 460.27.04    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN Xp            On   | 00000000:3D:00.0 Off |                  N/A |\n",
      "| 33%   55C    P2   119W / 250W |   5061MiB / 12196MiB |     70%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  TITAN Xp            On   | 00000000:3E:00.0 Off |                  N/A |\n",
      "| 23%   21C    P8     9W / 250W |      1MiB / 12196MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  TITAN Xp            On   | 00000000:60:00.0 Off |                  N/A |\n",
      "| 23%   22C    P8     8W / 250W |      1MiB / 12196MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  TITAN Xp            On   | 00000000:61:00.0 Off |                  N/A |\n",
      "| 23%   24C    P8     8W / 250W |      1MiB / 12196MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  TITAN Xp            On   | 00000000:B1:00.0 Off |                  N/A |\n",
      "| 23%   22C    P8     8W / 250W |      1MiB / 12196MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  TITAN Xp            On   | 00000000:B2:00.0 Off |                  N/A |\n",
      "| 23%   23C    P8     8W / 250W |      1MiB / 12196MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  TITAN Xp            On   | 00000000:DA:00.0 Off |                  N/A |\n",
      "| 23%   21C    P8     8W / 250W |      1MiB / 12196MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  TITAN Xp            On   | 00000000:DB:00.0 Off |                  N/A |\n",
      "| 23%   20C    P8     8W / 250W |      1MiB / 12196MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     40485      C   python3                          5057MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "40485: Operation not permitted\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_loader.dataset"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "image captioning deep learning workshop.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "conda-root-py",
   "display_name": "Python [conda env:root] *",
   "language": "python"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}